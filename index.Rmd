---
title: "Open Case Studies: Disparities in Youth Disconnection"
css: style.css
output:
  html_document:
    self_contained: yes
    code_download: yes
    highlight: tango
    number_sections: no
    theme: cosmo
    toc: yes
    toc_float: yes
  pdf_document:
    toc: yes
  word_document:
    toc: yes

---

<style>
#TOC {
  background: url("https://opencasestudies.github.io/img/logo.jpg");
  background-size: contain;
  padding-top: 240px !important;
  background-repeat: no-repeat;
}
</style>


<!-- Open all links in new tab-->  
<base target="_blank"/>  


```{r setup, include=FALSE}
knitr::opts_chunk$set(include = TRUE, comment = NA, echo = TRUE,
                      message = FALSE, warning = FALSE, cache = FALSE,
                      fig.align = "center", out.width = '90%')
library(here)
library(knitr)
```


#### {.outline }
```{r, echo = FALSE, out.width = "600 px"}
knitr::include_graphics(here::here("img", "mainplot.png"))
```

####



## {.disclaimer_block}

**Disclaimer**: The purpose of the [Open Case Studies](https://opencasestudies.github.io) project is **to demonstrate the use of various data science methods, tools, and software in the context of messy, real-world data**. A given case study does not cover all aspects of the research process, is not claiming to be the most appropriate way to analyze a given data set, and should not be used in the context of making policy decisions without external consultation from scientific experts. 


## {.license_block}

This work is licensed under the Creative Commons Attribution-NonCommercial 3.0 [(CC BY-NC 3.0)](https://creativecommons.org/licenses/by-nc/3.0/us/)  United States License.


## {.reference_block}

To cite this case study please use:

Wright, Carrie, and Ontiveros, Michael and Jager, Leah and Taub, Margaret and Hicks, Stephanie. (2020). https://github.com/opencasestudies/ocs-youth-disconnection-case-study. Disparities in Youth Disconnection (Version v1.0.0).

## **Motivation**
*** 

First, let's discuss the meaning of the term **"youth disconnection"**. 

According to [Measure of America](https://www.ssrc.org/programs/view/moa/) (a nonpartisan project of the nonprofit [Social Science Research Council](https://www.ssrc.org/) that is focused on opportunity in the United States), disconnected youth are:

> "young people between the ages of **16 and 24** who are **neither working nor in school**"

The group states that such disconnection hinders these individuals to acquire skills and create relationships necessary to have a successful adulthood. 

The group goes on to state that:

> "people who experience a period of disconnection as young adults go on to **earn less** and are **less likely** to be **employed, own a home, or report good health** by the time they reach their thirties"

Disconnected youth are also referred to as **opportunity youth**, which has the added positive connotation that promoting such individuals can be beneficial not only for these individuals, but also for their communities and for society. 

**Good news**: According to this [report](https://ssrc-static.s3.amazonaws.com/moa/Making%20the%20Connection.pdf), the youth disconnection is generally showing decreasing trends for the past 7 years. 

**Bad news**: The same report shows **racial and ethnic disparities**, where some groups are showing **increased rates of disconnection**.

In this case study, we will expand beyond the [Measure of America annual report](https://ssrc-static.s3.amazonaws.com/moa/Making%20the%20Connection.pdf) to take a deeper look at differences in disconnection between different subgroups of youths. 
Identifying youths particularly at risk or disconnected, can help inform the design of targeted prevention and re-engagement strategies.
To do this, we use the following [article](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6243446/) as our motivation for this case study. 

#### {.reference_block}

Mendelson, T., Mmari, K., Blum, R. W., Catalano, R. F. & Brindis, C. D. Opportunity Youth: Insights and Opportunities for a Public Health Approach to Reengage Disconnected Teenagers and Young Adults. *Public Health Rep* 133, 54S-64S (2018).

####
 
The [article](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6243446/) describes strategies for prevention of disconnection and re-engagement of disconnected youth and how such interventions could greatly positively impact opportunity youth for the entire trajectory of their lives and for future generations. 
It also points out that indeed there are disparities among different racial/ethnic groups.


## **Main Questions**
*** 

#### {.main_question_block}
<b><u> Our main questions: </u></b>

1) How have youth disconnection rates in American youth changed since 2008?   
2) In particular, how has this changed for different gender and ethnic groups? Are any groups particularly disconnected? 

####

## **Learning Objectives** 
*** 

In this case study, we will demonstrate how to import and wrangle data available in a <u>**P**</u>ortable <u>**D**</u>ocument <u>**F**</u>ormat (**PDF**). We will especially focus on using packages and functions from the [`tidyverse`](https://www.tidyverse.org/), such as `dplyr`, `ggplot2`. The tidyverse is a library of packages created by RStudio. While some students may be familiar with previous R programming packages, these packages make data science in R more legible and intuitive.

The skills, methods, and concepts that students will be familiar with by the end of this case study are:

<u>**Data science Learning Objectives:**</u>

1. Importing text from PDF files using images and the `magick` package  
2. Apply action verbs in `dplyr` for data wrangling  
3. How to reshape data by pivoting between "long" and "wide" formats and separating columns into additional columns (`tidyr`)  
4. How to fill in data based on previous values (`tidyr`)
5. How to create data visualizations with `ggplot2` that are in a similar style to an existing image    
6. How to add images to plots using `cowplot`
7. How to create effective bar plots to for multiple comparisons, including adding gaps between bars in bar plots, adding figure legends to the plot area, and adding comparison lines (`ggplot2`)

<u>**Statistical Learning Objectives:**</u>  

1. Implementation of the Mann-Kendall trend test  
2. Interpretation of the Mann-Kendall trend test 
3. Difference between linear regression and Mann-Kendall trend test


```{r, out.width = "20%", echo = FALSE, fig.align ="center"}
include_graphics("https://tidyverse.tidyverse.org/logo.png")
```

*** 

We will begin by loading the packages that we will need:

```{r}
library(here)
library(pdftools)
library(tesseract)
library(magick)
library(knitr)
library(dplyr)
library(stringr)
library(magrittr)
library(tidyr)
library(tibble)
library(ggplot2)
library(directlabels)
library(cowplot)
library(forcats)
library(Kendall)
library(patchwork)
```



 Package   | Use in this case study                                                                      
---------- |-------------
[here](https://github.com/jennybc/here_here){target="_blank"}       | to easily load and save data  
[pdftools](https://readr.tidyverse.org/)      | to import PDF documents  
[tesseract]() | AVOCADO: need to add details for `tesseract` (I could not compile until i installed and loaded it)
[magick](https://cran.r-project.org/web/packages/magick/vignettes/intro.html#Kernel_convolution)      | for importing images and extracting text from images   
[knitr](https://yihui.org/knitr/) |  for showing images in reports    
[dplyr](https://dplyr.tidyverse.org/){target="_blank"}      | to filter, subset, join, add rows to, and modify the data   
[stringr](https://stringr.tidyverse.org/){target="_blank"}      | to manipulate strings  
[magrittr](https://magrittr.tidyverse.org/){target="_blank"}      | to pipe sequential commands 
[tidyr](https://tidyr.tidyverse.org/){target="_blank"}      | to change the shape or format of tibbles to wide and long, to drop rows with `NA` values, to separate a column into additional columns, and to fill out values based on previous values   
[tibble](https://tibble.tidyverse.org/){target="_blank"}      | to create tibbles    
[ggplot2](https://ggplot2.tidyverse.org/){target="_blank"}      | to create plots  
[directlabels](http://directlabels.r-forge.r-project.org/docs/index.html){target="_blank"}      | to add labels directly to lines in plots  
[cowplot](https://cran.r-project.org/web/packages/cowplot/vignettes/introduction.html){target="_blank"}      | to add images to plots 
[forcats](https://forcats.tidyverse.org/){target="_blank"}      | to reorder factor for plot
[kendall](https://cran.r-project.org/web/packages/Kendall/Kendall.pdf) | to implement the Mann-Kendall trend test in R   
[patchwork](https://github.com/thomasp85/patchwork) | to combine plots



The first time we use a function, we will use the `::` to indicate which package we are using. Unless we have overlapping function names, this is not necessary, but we will include it here to be informative about where the functions we will use come from.

## **Context**
*** 

So how does youth disconnection happen and what impact does it have?


There are many known **risk factors**, which have been identified in a variety of contexts (from family, friends, school, community, society) including:  

 - poverty (disconnected youth are nearly twice as likely to live in poverty and receive Medicaid)  
 - racial/ethnic disparities (findings suggest that these persist even when controlling for income)  
 - residential environment (in 2016 while 11.7% was the national average, 24% of people age 16-24 in the rural South were disconnected)  
 - poor academic performance  
 - poor mental health  
 - substance use disorders  
 - parental unemployment  
 - trauma exposure  
 - association with socially deviant peers 
 - school policies such as "one strike and you're out" - which is a zero tolerance school expulsion policy and shown to increase dropouts and incarceration rates
 
 These risk factors make it more likely for young people to miss out on education, training, and networking that can act as a foundation for a successful career.
 
There are also many known **negative consequences** associated with youth disconnection including but not limited to:

- chronic unemployment
- poverty
- poor mental health and poor general health (in a 2002 study - youths disconnected for 6 or more months were 3 times more likely to develop depression or other mental health disorder)
- criminal behavior (in a 2002 study - youths disconnected for 6 or more months were 5 times more likely to have a criminal record)
- incarceration
- early mortality 


```{r, out.width= "400px",echo=FALSE}
knitr::include_graphics(here::here("img", "jon-tyson-ajzN2AYNi1U-unsplash.jpg"))
```
<span>Photo by <a href="https://unsplash.com/@jontyson?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText">Jon Tyson</a> on <a href="https://unsplash.com/s/photos/unemployment?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText">Unsplash</a></span>

Furthermore, in 2012 it was estimated that each disconnected youth costs taxpayers $250,000 during a lifetime due to lost tax revenue and costs for social services, heath care and criminal justice.

Youth disconnection can be described as a continuum, as some youths will be disconnected for a brief time, while others are chronically disconnected. Additionally, while an individual who is out of school and work and also has poor support from the relationships of others may be further disconnected than an individual who has social support.

Here is an illustration of risk factors, protective factors and the continuum of disconnection:

```{r, echo = FALSE, out.width = "800 px"}
knitr::include_graphics(here::here("img", "risk_factors.png"))
```

##### [[source]](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6243446/pdf/10.1177_0033354918799344.pdf) 


### Strategies to mitigate youth disconnection

Many programs have identified useful strategies in re-engaging disconnected youth or preventing disconnection of youth. 

Generally speaking, most programs focus on re-engagement strategies, however, prevention strategies are likely to be just as important. 

Research suggests that active involvement with at risk youth from infancy and across multiple developmental stages through young adulthood would be the most beneficial.

In fact, the quality of parental care-giving of infants age 6-24 months has actually been shown to be a predictor of high school dropout rates! Thus early interventions may be very important and consistent continual engagement may prevent further disconnection of youths.

Prevention strategies include:  

- Preschool  
- [The Good Behavior Game](https://www.goodbehaviorgame.org/good-behavior-game-what-is-it)  
- Strengthening family and community connections  
- Promoting academic and career engagement  
- Life skills training for youths and families  
- Education about substance abuse  
- [Cognitive behavioral therapy](https://www.apa.org/ptsd-guideline/patients-and-families/cognitive-behavioral)    


#### {.emphasis_block}

Want to learn more about how to prevent and mitigate youth disconnection?  
Or do you know youths who are disconnected?  

See [the program directory at youth.gov](https://youth.gov/evidence-innovation/program-directory?keywords=&field_pd_factors_risks_tid=413&field_pd_factors_protective_tid=All) and [this program listing](https://goc.maryland.gov/wp-content/uploads/sites/8/2015/10/Program-Models-for-Serving-Opportunity-Youth.pdf) focused on Maryland but including other locations for listings of programs dedicated to re-engaging disconnected youth or preventing disconnection.

Also, see [The Center for Communities That Care](https://www.communitiesthatcare.net/) and the
[PROSPER program](https://extension.psu.edu/promoting-school-community-university-partnerships-to-enhance-resilience) for particular examples.

####


The statistics used in this section came from this [article](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6243446/).

## **Limitations**
*** 

There are some important considerations regarding this data analysis to keep in mind: 

1. This data used in the [Measure of America](https://www.ssrc.org/programs/view/moa/) project is derived from [American Community Survey (ASC)](https://www.census.gov/programs-surveys/acs), which excludes or under-represents certain opportunity youth groups, such as youths in the juvenile justice system, youths in the foster care system, and homeless youths as the survey is conducted on households. Furthermore, youths who may be more disconnected for other reasons besides not being in work or school, such as dealing with the added challenge of being a teenage mother, or being abused are not available in this dataset. Thus, this data likely underestimates youth disconnection rates. Furthermore, the data used in these reports are collected from human participants; this presents the *potential* for [response bias](https://en.wikipedia.org/wiki/Response_bias), as there is the *potential* that participants in the [sampling frame](https://en.wikipedia.org/wiki/Sampling_frame?oldformat=true){target="_blank"} may for a variety of reasons report inaccurate information. 

2. Data about certain group [intersections](https://www.vox.com/the-highlight/2019/5/20/18542843/intersectionality-conservatism-law-race-gender-discrimination) (meaning for example individuals of a particular gender and ethnicity) or particular groups in general such as specific ethnicities or gender or sexual identity groups such as [LGBTQIA+](https://www.uis.edu/gendersexualitystudentservices/about/lgbtqaterminology/) (lesbian/gay/bisexual/pansexual/transgender/genderqueer/queer,questioning/intersexed/agender/asexual) is unfortunately not available in the data used in this analysis and in most research about this topic. Luckily however, recent years of the [ACS survey](https://www.census.gov/programs-surveys/acs) has more detailed information about a greater number of racial and ethnic groups and racial/ethnic intersections.

3. The statistical procedures we are using may be overly simplistic. *In all data analysis, we need to be wary about deriving meaning from the statistical procedures we use*.

4. Using image processing tools can be very helpful. The manner in which data is obtained with image processing tools is what we would describe as a **black box process**, <u>*a process with known inputs and outputs but unknown mechanics*</u>. Because we are unaware of how our outputs are generated from our inputs, we need to be wary of the output. With the small output we are creating in this case study, a visual inspection should suffice. 


## **What are the data?**
*** 

In this case study we will be using data related to youth disconnection from the two following reports from the [Measure of America](https://www.ssrc.org/programs/view/moa/)  project:

> Measure of America is a nonpartisan project of the nonprofit [Social Science Research Council](https://www.ssrc.org/)  founded in 2007 to create easy-to-use yet methodologically sound tools for understanding well-being and opportunity in America. Through reports, interactive apps, and custom-built dashboards, Measure of America works with partners to breathe life into numbers, using data to identify areas of highest need, pinpoint levers for change, and track progress over time.

1. Lewis, Kristen. [Making the Connection: Transportation and Youth Disconnection](https://ssrc-static.s3.amazonaws.com/moa/Making%20the%20Connection.pdf) . New York: Measure of America, Social Science Research Council, 2019.  (Data up to 2017)

```{r, out.width="400px", echo = FALSE}
knitr::include_graphics(here::here("img", "Making_the_Connection.png"))
```

2. Lewis, Kristen. [A Decade Undone: Youth Disconnection in the Age of Coronavirus](https://ssrc-static.s3.amazonaws.com/moa/ADecadeUndone.pdf). New York: Measure of America, Social Science Research Council, 2020. (Data up to 2018)

```{r, out.width="400px", echo = FALSE}
knitr::include_graphics(here::here("img", "A_Decade_Undone.png"))
```

The data used in these reports comes from the [American Community Survey (ASC)](https://www.census.gov/programs-surveys/acs), which is the largest survey conducted by the United States Census Bureau. The survey started in 2005 and collects data for 3.5 million households annually. Data is collected about ancestry, citizenship, income, employment, disability among many other aspects. See [here](https://en.wikipedia.org/wiki/American_Community_Survey) for more detailed information about the survey.

According to [Wikipedia](https://en.wikipedia.org/wiki/American_Community_Survey):

> Data is collected by internet, mail, telephone interviews and in-person interviews... About 95 percent of households across all response modes ultimately respond... ACS responses are confidential... and "immune from legal process".

AVOCADO: curious -- what's the purpose of including the "immune from legal process" portion?

> It is a mandatory survey, it is governed by federal laws that could impose a fine of as much as $5,000 for refusing to participate.


We are particularly interested in the following tables on the last page of the [Measure of America 2019 report](https://ssrc-static.s3.amazonaws.com/moa/Making%20the%20Connection.pdf):

```{r, echo = FALSE, gender_race_eth_2019}
knitr::include_graphics(here::here("img", "gender_race_ethnicity_overview.png"))
```

Also, we are particularly interested in the tables on the following pages from the [Measure of America 2020 report](https://ssrc-static.s3.amazonaws.com/moa/ADecadeUndone.pdf):


```{r, echo = FALSE}
knitr::include_graphics(here::here("img", "latinx_2018_overview.png"))
```

```{r, echo = FALSE}
knitr::include_graphics(here::here("img", "asian_2018_overview.png"))
```



## **Data Import**
*** 

### Importing with `pdftools`

One way to import data from a pdf is to use the `pdf_text()` function of the `pdftools` package. The `here()` function in the `here` package allows us to specify the path or location of the document that we want to import, starting from the directory where a `.Rproj` file is located. In this case, we will import the `Making_the_Connection.pdf` in the `docs` directory. (*Note this is only the case if you pull the repository from github.*)

```{r}
pdf_tools_example <- 
  pdftools::pdf_text(here::here("docs","Making_the_Connection.pdf"))
```

We can take a look at the output for the page with our table of interests by simply using brackets `[]` around the page number. The page we are interested in (although labeled 39 in the report) is the 44th page, which looks like this:

```{r, gender_race_eth_2019, echo=FALSE}
```

#### {.scrollable }
```{r}
# Scroll through the output!
pdf_tools_example[44]
```
####

From the output, it's clear that a relatively large amount of manipulation will be required to wrangle this data. If you are interested in learning more about the `pdftools` package, please see this [case study](https://opencasestudies.github.io/ocs-bp-rural-and-urban-obesity/) and this [case study](https://opencasestudies.github.io/ocs-bp-rural-and-urban-obesity/).

AVOCADO: the two links above are the same. Assuming you meant to change one. 

#### {.think_question_block}
<b><u> Question Opportunity </u></b>

If are familiar with `pdftools`, spend 3-5 minutes trying to extract the multiple tables from page 44. 

####

While you might find the question above to not be impossible using the `pdftools` package, you might find it a bit challenging because of how the multiple tables are displayed on this page.

While our output may be reproducible, this process may be too time consuming.

Let's consider alternative approaches to importing these data. 

Next, we will demonstrate how to produce reproducible tables with image processing software in `R` using a package called `magick` which allows for the extraction of text from images. The **advantage** of this option, is that we can take a screenshot of just a piece of the page to wrangle. 


### Importing with `magick`

We will now import the data using the `magick` package which allows for the importation of data from images. 

First, we will take a screenshot (specifically create a `.png` file) of the top part of the gender, race, and ethnicity table on the last page of the [2019 Measure of America Report](https://ssrc-static.s3.amazonaws.com/moa/Making%20the%20Connection.pdf). We are only interested in the percentage of disconnection across the years, so we don't need our screenshot to include the last couple of columns.

Here, we show what this file looks like in this rendered rmarkdown website by using the `include_graphics()` function of the `knitr` package.

```{r}
knitr::include_graphics(here::here("img", "Major_ethnic_groups_screenshot.png"))
```

Next, we use the `image_read()` function of the `magick` package to import this image.

Then, we use the `image_info()` function to make sure that the import worked and to get information about the size, format and color of the image.

```{r, dpi = 1000}
Major_racial_ethnic_groups <- 
  magick::image_read(here::here("img", "Major_ethnic_groups_screenshot.png"))

magick::image_info(Major_racial_ethnic_groups)
```

We get some basic information, such as the file format is a PNG, the width and height of the image, etc. Let's take a closer look at our image in R! Now that we have imported it to see this image, we simply need to type the name of the image.

```{r}
Major_racial_ethnic_groups
```

Nice!


Let's import a couple more images just for fun. Here we will import an image directly from a URL.

```{r, out.width="20%"}
ggplot2_url <- "https://d33wubrfki0l68.cloudfront.net/2c6239d311be6d037c251c71c3902792f8c4ddd2/12f67/css/images/hex/ggplot2.png"

ggplot2_logo <- image_read(ggplot2_url)
ggplot2_logo
```

Now we will use the `image_ocr()` function of the `magick` package to extract the text from the OCS logo image. This function uses the `tesseract` package which has tools for [optical character recognition (OCR)](https://en.wikipedia.org/wiki/Optical_character_recognition), hence the `ocr` in the function name. This allows the function to identify text in images. These OCR tools have often been developed using [machine learning](https://en.wikipedia.org/wiki/Machine_learning) in which an algorithm was trained on images with and without text to "learn" to recognize text. See [here](https://towardsdatascience.com/a-gentle-introduction-to-ocr-ee1469a201aa) to learn more about how OCR works.

```{r}
magick::image_ocr(ggplot2_logo)
```

Awesome! We were able to extract text from this hex sticker!

One thing to keep in mind is that this doesn't always work. Unusual font, angles text, or particular colors can be difficult for the OCR to recognize.

Here is an example that does not work with the current version of `magick`:

```{r, out.width="20%"}
tidyverse_url <- "https://tidyverse.tidyverse.org/logo.png"

tidyverse_logo <- image_read(tidyverse_url)
tidyverse_logo
magick::image_ocr(tidyverse_logo)
```

This is likely do to the background on this particular hex sticker. So sometimes this process requires a bit of trial and error.



## **Data Wrangling**
*** 
Now let's try extracting the text from our image files.

### Major Racial and Ethnic Groups

The first image we imported looks like this. 

```{r}
Major_racial_ethnic_groups
```

Now we will extract the text! 

```{r}
major_groups <- magick::image_ocr(Major_racial_ethnic_groups)
major_groups
```
This looks like it worked fairly well!

We appear to have lost the column names (likely because of the different background color) but the values look pretty good.

You may notice that there are lots of `\n` values in the text from our image. These are [**newline**](https://en.wikipedia.org/wiki/Newline?oldformat=true) characters, which denote the end of a line of text and the start of a new line of text.  

To deal with these, we use the `str_split()` function of the `stringr` package to split based on the `\n` characters in the output, which turns out to be a list. Then, we can "unlist" the output using the base R `unlist()` function. By base R, we mean that the function it is loaded automatically in an R session. Finally, we use the `as_tibble()` function of the `tibble` package to convert the data into [tibble](https://tibble.tidyverse.org/) format, which is the tidyverse version of a data frame. This will allow us to see the values in the table much better.

To do all of these sequential steps efficiently we will use a method called piping.

***
<details> <summary>Click here if you are unfamiliar with piping in R, which uses this `%>%` operator.</summary>  

By [piping](https://cran.r-project.org/web/packages/magrittr/vignettes/magrittr.html) we mean using the `%>%` pipe operator which is accessible after loading the `tidyverse` or several of the packages within the tidyverse like `dplyr` because they load the [`magrittr` package](https://cran.r-project.org/web/packages/magrittr/vignettes/magrittr.html). 
This allows us to perform multiple sequential steps on one data input.   
</details>  
***

```{r}
 major_groups <- 
  major_groups %>%
  stringr::str_split(pattern ="\n") %>%
  unlist() %>%
  tibble::as_tibble()
```

#### {.scrollable }
```{r}
major_groups %>% 
  print(n = 20)

```
####


OK, this looks pretty good!

The only issue is that some values appear to be missing a decimal point. 

No worries though, we can modify the entire table in a reproducible way to get those decimal places back. However, first we need to do some other wrangling steps first.

First, let's separate the first column about ethnicities with the values in the subsequent columns. We can do so using the `separate()` function of the `tidyr` package based on [regular expressions](https://en.wikipedia.org/wiki/Regular_expression). Regular expressions (abbreviated regex) are notation shortcuts that describe patterns in character strings. See [here](https://github.com/rstudio/cheatsheets/raw/master/strings.pdf) for an RStudio cheetsheat about them.

We want to separate by instances where a letter is followed by a space and then a number. 

We can specify any letter by using the regex `[:alpha:]` notation and any number by using the regex `[:digit]` notation. We could have listed every letter that we saw the first column ending with like so `s|e|E|O|K|N` but this would not be as reproducible (meaning maybe this would not work as well next year if a new group were added that ended in a different letter), and we might make a mistake. This is why the regex are so useful. 

```{r, echo = FALSE, out.width= "40%"}
knitr::include_graphics(here::here("img", "alpha.png"))
```
We can indicate that we want a space by using this regex:

```{r, echo = FALSE, out.width= "40%"}
knitr::include_graphics(here::here("img", "regex.png"))
```


Now to specify that we want to see a letter first followed by a space, followed by a digit, we need to use a look around: 

```{r, echo = FALSE}
knitr::include_graphics(here::here("img", "look_around.png"))
```

We will use the "preceded by" and "followed by" look arounds. Thus `(?<=[:alpha:])` stands for any letter that appears before a space `\\s` that is followed by any digit `(?=[0-9])`. Altogether the pattern we want to separate by looks like this: `"(?<=[:alpha:])\\s(?=[0-9])"`.


Now to separate the value column into two columns, we can use the `separate()` function of the `tidyr` package to do this. This will allow us to not only split the rows by our regex expression, but also to create column names.

There are three important arguments for the `separate()` function:  
- `col` - this specifies what column you are separating   
- `into` - this specifies the names of the new columns you are creating  
- `sep` - this specifies what character string to look for to separate by  

Thus we will separate the `value` column into `Group` and `years` columns.

```{r}
major_groups <- 
  major_groups %>%
  tidyr::separate(col = value, 
                  into = c("Group", "Years"), 
                  sep = "(?<=[:alpha:])\\s(?=[0-9])")

major_groups
```

Looks good!

Let's also get rid of the all caps for the major categories of the `Group` column. We can convert the words to only capitalize the first letter using the `str_to_title()` function of the `stringr` package. To specifically modify the `Group` column we can use the `mutate` function of the `dplyr` package. 

We are also going to use a special pipe operator (`%<>%`) from the [`magrittr` package](https://cran.r-project.org/web/packages/magrittr/vignettes/magrittr.html) called the compound assignment pipe-operator or sometimes the double pipe operator. 

This allows us to use the `major_groups` as our input and reassign it at the end after all the subsequent steps have been performed, although in this case it is only one step.

```{r}
major_groups %<>%
  mutate(Group = stringr::str_to_title(Group))

major_groups
```

Nice! That looks better.

For the year data we would like to try splitting the strings for each row into different columns based on a space. Currently all the data is listed in one column called `Years`.

We can use the `separate()` function of the `tidyr` package again to do this. This will allow us to split the rows by spaces, as well as provide names for the new columns. 


```{r}
major_groups %<>%
  tidyr::separate(col = Years, 
                  into = c("2008", "2010", "2012", "2014", "2016", "2017"), 
                  sep = " ")

major_groups
```



Looks pretty good!

We appear to have an empty row at the very end. Since all the values are `NA`, we can use the `drop_na()` function  of the `tidyr` package to remove it.

```{r}
major_groups %<>%
  tidyr::drop_na()

major_groups
```

Great, now we have 18 rows.

It's important to look very carefully at the text. Again, there are some values missing a decimal place. For example the row where the `Group` value is `Asian`, the third and fourth values are missing a decimal place.

AVOCADO: would it make more sense to talk about "Click here for an explanation about data types in R and about character strings" here instead of below? 

Looking at the original table we see that even values like 10 are represented as 10.0. 

To fix this, we will remove all decimals (which is sort of like multiplying all values that do have a decimal by 10) and then we will multiply all values by .01 to add the decimals back. 
Here, we use the `mutate()` function combined with the `across()` function, which allows us to specify the columns we want to perform a function on. We want to do this for all the year columns, so we can exclude the `Group` column by using a minus sign operator `-` in the `.cols` argument of the `across()` function like so: `mutate(across(.cols = -Group))`

Finally, we will use the `str_remove()` function of the `stringr` package to find instances of "." and remove them. Since "." is a regex, and indicates any character string, thus we need "\\" to have R interpret a decimal or a period instead, as we can see from the RStudio cheat sheets:

```{r, echo = FALSE, out.width="60%"}
knitr::include_graphics(here::here("img", "regex_period.png"))
```

To pass the data from all the columns except our `Group` variable into our `str_remove()` function, we need to use the `.` notation as a replacement for the data that we specified by the `.cols`argument and we need to use `~` in front of the function name. 

```{r}
major_groups %<>%
    mutate(across(.cols = -Group, 
                  ~str_remove(string = ., pattern = "\\.")))

major_groups
```


Great, now in order to multiply each value by 0.1 we need to first make the values numeric. Currently we can tell that they are character strings based on the `<char>` values listed under each column name.

*** 
<details> <summary> Click here for an explanation about data types in R and about character strings. </summary>
*** 

There are several classes of data in R programming. 
Character is one of these classes. 
A character string is an individual data value made up of characters. 
This can be a paragraph, like the legend for the table, or it can be a single letter or number like the letter `"a"` or the number `"3"`. 
If data are of class character, than the numeric values will not be processed like a numeric value in a mathematical sense. 
If you want your numeric values to be interpreted that way, they need to be converted to a numeric class. 
The options typically used are integer (which has no decimal place) and double precision (which has a decimal place). 

</details>


To convert our values to be numeric we can use the base `as.numeric()` function. Again we will use `mutate()` and `across()`. Since this function doesn't require any arguments, we don't need to specify it's input like we just did for the `str_remove()` but we could do so as shown below.

```{r}
major_groups %<>%
   mutate(across(.cols = -Group, as.numeric))

#this is equivalent:

#major_groups %<>%
#   mutate(across(.cols = -Group, ~as.numeric(.)))

major_groups
```

Great, we can see that the year variables are now numeric as they are now type double as indicated by the `<dbl>` below each column name. See the above section about data types if you are unfamiliar with type double.

OK, now we can multiply each value by 0.1 to add our decimal points back and get back to the original values.

```{r}

major_groups %<>%
    mutate(across(.cols = -Group, ~ . * 0.1))

major_groups

```

Now is a good time to double check that our table looks like what we expect.


```{r, echo = FALSE}
knitr::include_graphics(here::here("img", "gender_race_ethnicity.png"))
```

Looks good!

We also want to add a couple of variables about `Race_Ethnicity` and `Gender` so that we can select across groups later.  We can use the `recode()` function of the `dplyr` package to change specific values, as we create a new `Race_Ethnicity` variable from the `Group` variable. For the Data for all of the US we want the `Race_Ethnicity` variable values to be `"All_races"`.

```{r}
major_groups %<>% 
  mutate(Race_Ethnicity = 
           recode(Group, "United States" = "All_races",
                                "Female" = "All_races",
                                  "Male" = "All_races"))

head(major_groups)
```


We also want to remove Male and Female from this "Race_Ethnicity" variable, We can do so using the `str_remove()` function of the `stringr` package. Importantly, we are also removing the space before "Female" and "Male.

```{r}
major_groups %<>% 
  mutate(Race_Ethnicity = str_remove(string = Race_Ethnicity,  
                                    pattern = " Female| Male"))

head(major_groups)
```

For the new `Gender` variable we would like to extract just the "Female" and "Male" text from the `Group` variable. The `str_extract()` function of the `stringr` package will do this, and it will give us an `NA` value for any rows where "Female" or "Male" were not present. We can then replace the `NA` values with  the text "All" to represent the total value for both male and female using the `replace_na()` function of the `tidyr()` package.

```{r}
major_groups %<>% 
  mutate(Gender = str_extract(string = Group, 
                              pattern = "Female|Male")) %>%
  mutate(Gender = replace_na(Gender, replace = "All"))

head(major_groups)
```

We would also like to replace Latino and Latina with Latinx. 
We can  use another `stringr` function for this. This function, `str_replace()` allows us to remove and replace a particular pattern. 

```{r}
major_groups %<>%
  mutate(across(.cols = c(Group, Race_Ethnicity), 
                ~str_replace(string = ., 
                            pattern = "Latino|Latina",
                        replacement = "Latinx")))
```

#### {.think_question_block}
<b><u> Question Opportunity </u></b>

Why is the `str_replace()` function in this case a better option than using the `recode()` function?

####

AVOCADO: does this need a hidden answer block?

```{r}
major_groups
```
Finally, we would like to change the shape of our table so that we have a new column that represents the year and a new column that represents the value for that year.

To do so we will be making our table "longer", meaning that it will have fewer columns and more rows. 
See [here](https://en.wikipedia.org/wiki/Wide_and_narrow_data) for more information about different table formats, typically referred to as wide and long or sometimes narrow.

We will use the `pivot_longer()` function of the `tidyr` package to change the shape of our table. 

There are 3 main arguments in this function:   

1. `cols` - specifies what columns to collapse  
2. `names_to` - specifies the name of the new column that will be created that will contain the column names of the columns you are collapsing  
3. `values_to` - specifies the name of the new column that will be created that will contain the values from the columns you are collapsing 

To specify that we want to collapse all the columns that have year values, we can chose those that contain the string `"20"` using the `contains()` function of `dplyr`. 

```{r}
major_groups_long <- major_groups %>%
  tidyr::pivot_longer(cols = contains("20"),
                  names_to = "Year",
                 values_to = "Percent",
              names_prefix = "Perc_") %>%
  dplyr::mutate(Year = as.numeric(Year))

major_groups_long
```


Excellent, now we need to do the same for the other two tables on this page:

```{r, echo  = FALSE}
knitr::include_graphics(here::here("img", "gender_race_ethnicity_overview.png"))
```


### Asian Subgroups 2017

Now let's do the same for the Asian subgroups table.

First we will start by importing a screenshot for this table without the header, as we did before. The name of the file for the screenshot is `asian_subgroups.png` and it is located in the `img` directory.

#### {.recall_code_question_block}
<b><u> Question Opportunity </u></b>

Can you recall the command to import an image into R using the `magick` package?

####


<details> <summary> Click here to reveal the code. </summary>


```{r, out.width}
asian_subgroups <- image_read(here::here("img", "asian_subgroups_2017.png"))
```

</details>


```{r, out.width="40%"}
asian_subgroups
```

#### {.recall_code_question_block}
<b><u> Question Opportunity </u></b>

Can you recall the command to extract the text from an image using the `magick` package? 

####

<details> <summary> Click here to reveal the code. </summary>

```{r}
asian_subgroups <- image_ocr(asian_subgroups)
```

</details>

```{r}
asian_subgroups
```


Now we again want to split the data into rows based on the newline regex. This is something we will continue to do for all the tables.

One option is to copy and paste code we wrote above each time. 
However, this is not very efficient and is error prone.
Alternatively, we can create a R function to accomplish this succinctly. 
Functions allow us to perform the same process on multiple data inputs. 
See [this other case study](https://opencasestudies.github.io/ocs-bloomberg-vaping-case-study/) for more details about how to write a function.

In general, the process of writing functions involves first specifying an input that is used within the function to create an output. In this case, the data input is `text`  which will be replaced by the actual image text that we are working on, and then used in the subsequent steps to wrangle the data. We will call our function `make_rows()`.

```{r}
make_rows <- function(text){
  text %>%
  str_split("\n") %>%
  unlist() %>%
  as_tibble()
}
```

Great! Now let's apply our function to the `asian_subgroups` data!

```{r}
asian_subgroups <- make_rows(asian_subgroups) 

asian_subgroups
```


As you can see, there are some strange values for some of the rows. For example the row that starts with `CHINESE MAle` has `AT` percentage of disconnected youth, and the row that should say `FILIPINO` says `S™S™~SS`.

AVOCADO: the 2nd example you listed is not show in the output of the first 10 rows. Did you want to do a scroll instead?

```{r, echo = FALSE}
temp <- 
  asian_subgroups %>%
  pull(value) %>%
  str_split(" ")

temp[8]
temp[28]
```

The rows with no values are possibly causing this issue. According to the report (PDF), these spaces are empty to denote that the estimates were unreliable for these groups.

So we will now import and extract text from three screenshots of this table where we stop just after the row that starts with `PAKISTANI` in the first image, and then an image of the Korean rows up to the next row with no values, and finally an image starting at the row that starts with `FILIPNO`.

AVOCADO: how did you know splitting the images up would work? Maybe more insights just as to how you knew to try that next? or why couldn't you just ignore those rows in the original table you imported before breaking it up into three images?

```{r, out.width= "60%"}
Asian_sub_A <- image_read(here("img", "asian_sub_A.png"))
Asian_sub_B <- image_read(here("img", "asian_sub_B.png"))
Asian_sub_C <- image_read(here("img", "asian_sub_C.png"))

Asian_sub_A
Asian_sub_B
Asian_sub_C 

```


```{r}
Asian_sub_A <- image_ocr(Asian_sub_A)
Asian_sub_B <- image_ocr(Asian_sub_B)
Asian_sub_C <- image_ocr(Asian_sub_C)

Asian_sub_A <- make_rows(Asian_sub_A)
Asian_sub_B <- make_rows(Asian_sub_B)
Asian_sub_C <- make_rows(Asian_sub_C)

Asian_sub_A 
Asian_sub_B
Asian_sub_C 
```

Much better!

We can now combine the objects with the `bind_rows()` function of the `dplyr` package, which will append each of these tibbles together one after the other.

```{r}
asian_sub_2017 <-bind_rows(Asian_sub_A, 
                           Asian_sub_B,
                           Asian_sub_C)

asian_sub_2017
```

Looks pretty good! 

Now we have similar wrangling steps to perform as we did previously and we will need to do the same for the Latinx subgroups table. So it is a good idea to make another function.

Even though we appear to have all of the decimal places for the values, we will include this in our function, just to make sure the data is correct.


#### {.explain_block}
<b><u> Question Opportunity </u></b>

Can you explain what each of the commands are doing within the function?

####

```{r}
clean_table <- function(table){
  table %>%
    separate(., col = value,
               into = c("Group", "Percentage"),
                sep =  "(?<=[:alpha:])\\s(?=[0-9])") %>% 
    drop_na() %>%
    mutate(Group = str_to_title(Group)) %>%
    mutate(Percentage = str_remove(string = Percentage,
                                   pattern = "\\.")) %>%
    separate(Percentage, c("Percent"), sep = " ") %>%
    mutate(Percent = as.numeric(Percent)) %>%
    mutate(Percent = Percent * 0.1) %>%
    mutate(Race_Ethnicity = recode(Group, 
                  "United States" = "All_races", 
                  "Female" = "All_races",
                  "Male" = "All_races")) %>%
    mutate(Race_Ethnicity = str_remove(string = Race_Ethnicity,  
                                       pattern = " Female| Male")) %>%
    mutate(Gender = str_extract(string = Group,
                                pattern ="Female|Male")) %>%
    mutate(Gender = replace_na(Gender, replace = "All"))
 
}
```



```{r}
asian_sub_2017 <- clean_table(table = asian_sub_2017)
asian_sub_2017
```

Great! This looks as we expected. 

#### {.think_question_block}
<b><u> Question Opportunity </u></b>

Why do we not need to use `pivot_longer()` with this data?

####


### Latinx Subgroups 2017


Recall that this is the table we want to wrangle:

```{r, out.width = "80%"}
knitr::include_graphics(here::here("img", "latinx2017.png"))
```

#### {.think_question_block}
<b><u> Question Opportunity </u></b>

Do you notice anything incorrect about this table?

####

AVOCADO: I was confused by this question and still not quite sure what the typo is? Maybe give an answer here or provide more explanation below? Still not sure if you are suggesting two rows were swapped or if it was something else?

Sometimes when wrangling text data, we will come across a typo. We need to determine how to respond to the typo and make note of it. It's often best to consult a secondary source to confirm that changes made are accurate. 

For the purposes of this case study, we will assume that the first of the two rows represents male disconnection rates in the Latino/a subgroup; this would be consistent with the ordering of genders in the previous subgroups. 

We will make sure to correct this typo when we can. 


After trial and error, two screenshots were determined best for importing this data. The names of the files for the screenshots are `latinx_sub_A.png` and `"latinx_sub_B.png`. They are located in the `img` directory.

#### {.recall_code_question_block}
<b><u> Question Opportunity </u></b>

Can you recall the commands to import and extract the data?

####


<details> <summary> Click here to reveal the code. </summary>

```{r}
latinx_imageA <- image_read(here::here("img", "latinx_sub_A.png"))
latinx_imageB <- image_read(here::here("img", "latinx_sub_B.png"))
latinx_imageC <- image_read(here::here("img", "latinx_sub_C.png"))

latinx_A <- image_ocr(latinx_imageA)
latinx_B <- image_ocr(latinx_imageB)
latinx_C <- image_ocr(latinx_imageC)

```

</details>



```{r, out.width="40%"}
latinx_imageA
latinx_A

latinx_imageB
latinx_B

latinx_imageC
latinx_C
```
We can combine the strings together using the `str_c()` function (which stands for string **collapse**) of the `stringr` package.

```{r}
latinx_sub_2017 <- stringr::str_c(latinx_A, latinx_B, latinx_C)

latinx_sub_2017
```


Now let's correct that typo.

#### {.think_question_block}
<b><u> Question Opportunity </u></b>

How might you do this?

####

<details> <summary> Click here to reveal the code. </summary>


```{r}
latinx_sub_2017 %<>% 
  str_replace(string = .,
             pattern = "DR, Cuban Female 15.7\nPR",
         replacement = "DR, Cuban Male 15.7\nPR")
```
</details>

```{r}
latinx_sub_2017
```

AVOCADO: OH i totally missed that. Yeah, definitely suggest more commentary above... :/ 


#### {.recall_code_question_block}
<b><u> Question Opportunity </u></b>

Can you recall the commands within our `make_rows()` function to separate the data into rows and create a tibble?

####

<details> <summary> Click here to reveal the code. </summary>

```{r}
latinx_sub_2017 %<>%
  str_split("\n") %>%
  unlist() %>%
  as_tibble()
```
</details>

```{r}
latinx_sub_2017
```
Now we can apply our function. 

```{r}
latinx_sub_2017 <- clean_table(table = latinx_sub_2017)
latinx_sub_2017
```


It looks like we've successfully corrected the typo!


Let's also replace the abbreviations for Puerto Rican and Dominican and let's replace Latino/Latina with Latinx.

#### {.think_question_block}
<b><u> Question Opportunity </u></b>

How might you do this using the `str_replace()` function?

####

<details> <summary> Click here to reveal the code. </summary>



```{r}
latinx_sub_2017 %<>%
  mutate(across(.cols = c(Group, Race_Ethnicity), 
           ~str_replace(string = .,
                       pattern = "Pr, Dr, Cuban",
                   replacement = "Puerto Rican, Dominican, Cuban")),
         across(.cols = c(Group,Race_Ethnicity),
          ~str_replace(string = .,
                      pattern = "Latino|Latina",
                  replacement = "Latinx")))
```

</details>


```{r}
latinx_sub_2017
```

Great!


Now we are ready to look at the data from 2018 for the Asian and Latinx subgroups from the other [report](https://ssrc-static.s3.amazonaws.com/moa/ADecadeUndone.pdf).


### Asian Subgroups 2018
***
Recall that this was the page with the table of interest for the Asian subgroups with 2018 data:

```{r, echo = FALSE, }
knitr::include_graphics(here::here("img", "asian_2018_overview.png"))
```

As you can see, the data for the subgroups is shown in the table but the overall data for Asians is located in the text.

We will use a screenshot of each to extract the data for this year.

```{r, echo = FALSE, out.width="30%"}
knitr::include_graphics(here::here("img/asian_subgroups_2018.png"))
```

```{r, echo = FALSE, out.width="80%"}
knitr::include_graphics(here::here("img", "asian_youth_excerpt.png"))
```
Trial and error indicated that again dividing the table into multiple screenshots improved the text extraction:

```{r}
asian_sub_2018_A <- image_read(here::here("img", "asian_sub_2018_A.png"))
asian_sub_2018_A <- image_ocr(asian_sub_2018_A)
asian_sub_2018_B <- image_read(here::here("img", "asian_sub_2018_B.png"))
asian_sub_2018_B <- image_ocr(asian_sub_2018_B)
asian_sub_2018 <-str_c(asian_sub_2018_A, asian_sub_2018_B)
```

```{r}
asian_sub_2018 <- make_rows(asian_sub_2018)

asian_sub_2018
```
Now we need to modify our function a bit for this new data. 

First, we now have colons `:` in our table that we will want to separate by. Unfortunately, the text in each row isn't extracted in the same way by the OCR. Some rows have only a space, while others have spaces around a colon; or for the row with `VIETNAM` we see a colon directly after the word followed by a space. Thus, we will modify our `seperate()` function with this change. We can specify that the separator between any letter and any digit should be either a space (`\\s`) or a colon with a space before and after it (`\\s:\\s`) using the or (`|`)operator. 

So this will look like this:

```{r}
asian_sub_2018 %>%
    separate(., col = value,
             into = c("Group", "Percent"), 
             sep =  "(?<=[:alpha:])\\s:\\s|\\s(?=[0-9])")

```
Then because of the row with `VIETNAM`, we will want to remove this colon using the `str_remove()` function like this:

```{r, eval = FALSE}
asian_sub_2018 %>%
    separate(., col = value, 
               into = c("Group", "Percent"), 
                sep = "(?<=[:alpha:])\\s:\\s|\\s(?=[0-9])") %>% 
    mutate(Group= str_remove(string = Group, pattern = ":"))

```

The other difference from the previous function, is that we want to fill in a new `Race_Ethnicity` variable with the previous rows. We can do so by first replacing "Men" or "Women" which with the or operator is ("Men|Women"), with "missing". Then we need to convert these to `NA` values using the `na_if()` function of the `dplyr` package, we just need to specify what column to modify and what value to change to `NA`. Finally we will then replace the `NA` values with the previous non-`NA` value using the `fill()` function of the `tidyr` package. Note that this does not work inside of the `mutate()` function. We just need to simply specify what column to modify and then the direction to replace values. In this case we want to replace in the downward direction using the previous values. 

This will look like this:

```{r}
asian_sub_2018 %>%
separate(., col = value, 
           into = c("Group", "Percent"), 
            sep = "(?<=[:alpha:])\\s:\\s|\\s(?=[0-9])") %>%
mutate(Group = str_remove(string = Group, pattern = ":")) %>%
mutate(Race_Ethnicity = str_replace(string = Group,
                                   pattern = "Men|Women", 
                               replacement = "missing")) %>%
head()

asian_sub_2018 %>%
separate(., col = value, 
           into = c("Group", "Percent"), 
            sep = "(?<=[:alpha:])\\s:\\s|\\s(?=[0-9])") %>%
mutate(Group = str_remove(string = Group, pattern = ":")) %>%
mutate(Race_Ethnicity = str_replace(string = Group,
                                   pattern = "Men|Women", 
                               replacement = "missing")) %>%
mutate(Race_Ethnicity = na_if(Race_Ethnicity, "missing")) %>%
head()

asian_sub_2018 %>%
separate(., col = value, 
           into = c("Group", "Percent"), 
            sep = "(?<=[:alpha:])\\s:\\s|\\s(?=[0-9])") %>%
mutate(Group = str_remove(string = Group, pattern = ":")) %>%
mutate(Race_Ethnicity = str_replace(string = Group,
                                   pattern = "Men|Women", 
                               replacement = "missing")) %>%
mutate(Race_Ethnicity = na_if(Race_Ethnicity, "missing")) %>%
fill(Race_Ethnicity, .direction = "down") %>%
head()
```

OK! Now, let's combine these pieces of our new function with the old pieces:

```{r}

clean_table <- function(table){
  table %>%
    separate(., col = value, 
               into = c("Group", "Percent"), 
                sep =  "(?<=[:alpha:])\\s:\\s|\\s(?=[0-9])") %>% 
    mutate(Group= str_remove(string = Group, 
                            pattern = ":")) %>%
    drop_na() %>%
    mutate(Group = str_to_title(string = Group)) %>%
    mutate(Percent = str_remove(string = Percent, 
                               pattern = "\\.")) %>%
    mutate(Percent = as.numeric(Percent)) %>%
    mutate(Percent = Percent * 0.1) %>%
    mutate(Race_Ethnicity = str_replace(string = Group,
                                       pattern = "Men|Women", 
                                   replacement = "missing")) %>%
    mutate(Race_Ethnicity = na_if(Race_Ethnicity, "missing")) %>%
    fill(Race_Ethnicity, .direction = "down") %>%
    mutate(Gender = str_extract(string = Group, 
                               pattern = "Men|Women")) %>%
    mutate(Gender = replace_na(Gender, replace = "All"))
}

```


```{r}
asian_sub_2018 <-clean_table(asian_sub_2018)
asian_sub_2018
```

Looking good!


Now we just need to add the data for all Asians from the text. 

```{r, echo = FALSE, out.width="60%"}
knitr::include_graphics(here::here("img", "asian_youth_excerpt.png"))
```

We can do this using the `add_row()` function of the `dplyr()` package.

```{r}
asian_sub_2018 %<>%
  add_row(Group = "Asian",
        Percent = 6.2,
 Race_Ethnicity = "Asian",
         Gender = "All") %>%
  add_row(Group = "Asian",
        Percent = 6.4,
 Race_Ethnicity = "Asian",
         Gender = "Men") %>%
  add_row(Group = "Asian",
        Percent = 6.1,
 Race_Ethnicity = "Asian",
         Gender = "Women")

asian_sub_2018
```

OK, now we just want to combine the 2018 data and the 2017 data for the Asian subgroups.

First let's add a variable for year to both. Using `mutate()` we can add a variable `Year` where all values are `2017` like so:

```{r}
asian_sub_2017 %<>%
  mutate(Year = 2017)
asian_sub_2017 
```

```{r}
asian_sub_2018 %<>%
  mutate(Year = 2018)
asian_sub_2018
```

You may notice that `Gender` is coded differently for the two years. Let's make this consistent now:

```{r}
asian_sub_2018 %<>%
  mutate(across(.cols = c(Gender, Group),
               ~ str_replace(string = ., 
                            pattern = "Men", 
                        replacement = "Male")),
         across(.cols = c(Gender, Group),
               ~ str_replace(string = ., 
                            pattern = "Women", 
                        replacement = "Female")))

```

We can combine these two tibbles using the `bind_rows()` function of `dplyr`. 

```{r}

asian_subgroups <- bind_rows(asian_sub_2017, asian_sub_2018)
```


```{r, echo = FALSE}
DT::datatable(asian_subgroups)
```



Notice that there are some cases where we only have one value for a particular group. For example, there are no male or female values for the Pakistani data.

We would like to have `NA` values for the comparable years/genders that are possible. We can fill out the rest of the table with `NA` values by performing the `pivot_wider()` and `pivot_longer()` functions sequentially like so:

```{r, eval=FALSE}

asian_subgroups %<>% 
select(-Group) %>%
pivot_wider(names_from = Year, values_from = Percent) %>%
pivot_longer(cols = -c(Race_Ethnicity, Gender), 
names_to ="Year" , 
values_to="Percent")
```


```{r, echo = FALSE}
DT::datatable(asian_subgroups)
```


Great, now we are ready to perform similar wrangling for the Latinx subgroups.

### Latinx Subgroups 2018 
***
Recall that this was the page with the table of interest for the Latinx subgroup 2018 data:

```{r, echo = FALSE, }
knitr::include_graphics(here::here("img", "latinx_2018_overview.png"))
```

In this case only a single image was needed:

```{r}
latinx_sub_2018 <- image_read(here::here("img", "latinx_subgroups_2018.png"))
latinx_sub_2018 <- image_ocr(latinx_sub_2018)
latinx_sub_2018
```

Let's first combine the South and Central American labels. Notice that there are multiple new line expressions in between and we don't see repeated `\n` characters elsewhere.
We can replace the pattern of exactly two `\n` (using `\n{2}` to specify exactly 2) or two newline regex with a space and colon in front with a single space.

```{r}
latinx_sub_2018 <- str_replace_all(string = latinx_sub_2018, 
                                  pattern = "\\s:\n{2}|\n{2}",                                               replacement = " ")
latinx_sub_2018 
```


```{r}
latinx_sub_2018 <- make_rows(latinx_sub_2018 )

latinx_sub_2018
```


```{r}
latinx_sub_2018 <-clean_table(latinx_sub_2018)
latinx_sub_2018
```


Again we will replace `Pr, Dr, Cuban`:
```{r}
latinx_sub_2018 %<>%
  mutate(Group = 
           str_replace(string = Group,
                      pattern = "Pr, Dr, Cuban",
                  replacement = "Puerto Rican, Dominican, Cuban"), 
          Race_Ethnicity = 
           str_replace(string = Race_Ethnicity,
                      pattern = "Pr, Dr, Cuban",
                  replacement = "Puerto Rican, Dominican, Cuban"))

```



We also want to add the total Latinx values according to the text:



```{r}
knitr::include_graphics(here::here("img", "latinx_youth_excerpt.png"))

```


#### {.recall_code_question_block}
<b><u> Question Opportunity </u></b>

Can you recall how to add additional rows?

####


<details> <summary> Click here to reveal the code. </summary>


```{r}
latinx_sub_2018 %<>%
  add_row(Group = "Latinx",
        Percent = 12.8,
 Race_Ethnicity = "Latinx",
         Gender = "All") %>%
  add_row(Group = "Latinx",
        Percent = 12.3,
 Race_Ethnicity = "Latinx",
         Gender = "Men") %>%
  add_row(Group = "Latinx",
        Percent = 13.3,
 Race_Ethnicity = "Latinx",
         Gender = "Women")
```

</details>


```{r}
tail(latinx_sub_2018)
```

And now we will recode gender like before to be consistent:

```{r}
latinx_sub_2018 %<>%
  mutate(across(.cols = c(Gender, Group),
                ~ str_replace(string = ., 
                             pattern = "Men", 
                         replacement = "Male")),
         across(.cols = c(Gender, Group),
                ~ str_replace(string = ., 
                             pattern = "Women", 
                         replacement = "Female")))

head(latinx_sub_2018)
```



Now we just need to combine all the data for the Latinx subgroups.


Again, first we will add a year variable to both the 2017 and 2018 data.

```{r}
latinx_sub_2017 %<>%
  mutate(Year = 2017)

latinx_sub_2018 %<>%
  mutate(Year = 2018)

latinx_subgroups <- bind_rows(latinx_sub_2017, latinx_sub_2018)
latinx_subgroups
```


Again, we would like to have `NA` values for the comparable years/genders that are possible. We will fill out the rest of the table with `NA` values by performing the `pivot_wider()` and `pivot_longer()` functions sequentially like so:

```{r, eval = FALSE}
latinx_subgroups %<>%
  select(-Group) %>%
  pivot_wider(names_from = Year, values_from = Percent) %>%
  pivot_longer(cols = -c(Race_Ethnicity, Gender), 
               names_to ="Year" , 
               values_to="Percent")
```

AVOCADO: maybe mention the use of `datatable` in the `DT` package here? 

```{r, echo = FALSE}
DT::datatable(asian_subgroups)
```

### Checking the data
***

OK, now let's make sure that our notations match across our different tables. For example in the first report the terms male and female where used, but in the second report men and women were used. Let's make sure everything is consistent now.

```{r}
major_groups_long
asian_subgroups
latinx_subgroups
```

Looks good! If you made it this far, you should pat yourself on your back. That was a lot of wrangling! 

```{r, echo= FALSE, eval = TRUE}
save(major_groups_long, asian_subgroups, latinx_subgroups,
     file = here::here("data", "wrangled_data.rda"))
```

## **Data Visualization**
*** 

Recall what our main questions were?


#### {.main_question_block}
<b><u> Our main questions: </u></b>

1) How have youth disconnection rates in American youth changed since 2008?   
2) In particular, how has this changed for different gender and ethnic groups? Are any groups particularly disconnected? 

####

Now that we have wrangled our data and made it easy to work with, let's create some visualizations to explore these questions.

```{r, echo = FALSE}
# To allow starting at this section:
load(here::here("data", "wrangled_data.rda"))
```



### Major Race and Ethnic Groups Plot
***

We are particularly interested in being able to reproduce the plot below from the report, as we would like to make similar looking plots.

```{r, out.width = "100%", echo = FALSE, fig.align ="center"}
include_graphics(here::here("img", "Making_the_Connection_plot.png"))
```

In general, it is very useful to learn how to reproduce the style of a plot.


There are color identifying websites such as [this](https://imagecolorpicker.com/en/).

Using one of these websites, we identify the [hex triplet](https://en.wikipedia.org/wiki/Web_colors?oldformat=true) code for the color used in the visualization included in the PDF : `#008393`.  Thus we will use this color our plot.


We can create a version of the above plot using the `ggplot2` package of the _tidyverse_ to create our plots.

***
<details><summary> Click here for an introduction about this package if you are  new to using `ggplot2` </summary>

The [ggplot2 package](http://ggplot2.tidyverse.org) is generally intuitive for beginners because it is based on a  [grammar of graphics](http://vita.had.co.nz/papers/layered-grammar.html) or the `gg` in `ggplot2`. 
The idea is that you can construct many sentences by learning just a few nouns, adjectives, and verbs. There are specific “words” that we will need to learn and once we do, you will be able to create (or “write”) hundreds of different plots.

The critical part to making graphics using `ggplot2` is the data needs to be in a _tidy_ format. 
Given that we have just spent time putting our data in _tidy_ format, we are primed to take advantage of all that `ggplot2` has to offer! 

We will show how it is easy to pipe _tidy_ data (output) as input to other functions that create plots. 
This all works because we are working 
within the _tidyverse_. 

**What is the `ggplot()` function?** 
As explained by Hadley Wickham:

> The grammar tells us that a statistical graphic is a mapping from data to aesthetic attributes (colour, shape, size) of geometric objects (points, lines, bars). The plot may also contain statistical transformations of the data and is drawn on a specific coordinates system.

`ggplot2` Terminology: 

- **ggplot** - the main function where you specify the dataset and variables to plot (this is where we define the `x` and
`y` variable names)
- **geoms** - geometric objects
    - e.g. `geom_point()`, `geom_bar()`, `geom_line()`, `geom_histogram()`
- **aes** - aesthetics
    - shape, transparency, color, fill, line types
- **scales** - define how your data will be plotted
    - continuous, discrete, log, etc

The function `aes()` is an aesthetic mapping function inside the `ggplot()` object. 
We use this function to specify plot attributes (e.g. `x` and `y` variable names) that will not change as we add more layers.  

Anything that goes in the `ggplot()` object becomes a global setting. 
From there, we use the `geom` objects to add more layers to the base `ggplot()` object. 
These will define what we are interested in illustrating using the data.

</details>

***

So for this first plot we just want to recreate the plot in the report. We will use the `major_groups_long` tibble and we will filter for omly the `"All"` values of the `Gender` column.
We also want to exclude the data for the United states. 
We can exclude the data from the US using the not equals `!=` operator like so:

```{r}
major_groups_long %>%
   filter(Gender == "All",
          Group != "United States")
```

The data from the tables only included up to 2017, but this will still allow us to create a similar plot.

Now we have the data ready to make the plot... but how do we actually start making the plot?

First, we start with the `ggplot()` function of the `ggplot2` package.

This function requires that the aesthetics `aes()` be specified. This involves choosing what variable will be plotted on the x-axis and the y axis. It also involves choosing variables to color or group our plot by. 

In our case, we want to plot the percent of disconnection on the y-axis (thus the `Percent` variable of the `major_groups_long` data) and the `Year` on the x-axis. We want to separate each racial or ethnic group to have their own points/lines. Thus we will use the `color` argument for this variable, as we intend to color our plot something other than black.

If we run the following code, we get an empty plot.

```{r, eval = FALSE}
major_groups_long %>%
   filter(Gender == "All",
          Group != "United States") %>%
ggplot(aes(x = Year, y = Percent, color = Race_Ethnicity))
```


The next thing we need to do is add `ggplot2` layers using the `+` operator to specify how we want the data to be displayed on our plot.

We would like both points and lines. We will use the `geom_point()` and `geom_line()` functions of the `ggplot2` package to do this. The layer we add first will be plotted first and so on with the next layers. We will also specify the size of these elements using the `size` argument.

```{r}
major_groups_long %>%
  filter(Gender == "All", Group != "United States") %>%
  ggplot(aes(x = Year, y = Percent, color = Race_Ethnicity)) +
    geom_line(size = 0.5) +
    geom_point(size = 3) 
```

OK, not bad, but we have quite a bit to work on to make the plot style match.

First, we will update the x-axis and y-axis, to look more similar to the plot from the report. We can specify the location of the tick marks using `breaks` argument of the `scale_x_continuous()` and `scale_y_continuous()` functions (also of the `ggplot2` package). These functions also allow for specification of the range or limits of the axis using the `limits` argument. We can use the base `seq()` function to create a sequence of numbers for each tick mark.
We will make the x-axis upper limit a bit larger to allow for the image of the figure and the label for each group.

Next, we will change the color to match the one that we identified, by using the `scale_color_manual()` function of the `ggplot2` package. This requires color values for each group. In our case, we have 5 groups, so we repeat this value 5 times using the base `rep()` function.

We will also change the overall look of the plot using the `theme_classic()` function. See [here](https://ggplot2.tidyverse.org/reference/ggtheme.html) for a list of options.

```{r}
 youth_plot <- 
  major_groups_long %>%
  filter(Gender == "All", Group != "United States") %>%
  ggplot(aes(x = Year, y = Percent, color = Race_Ethnicity)) +
    geom_line( size = 0.5) +
    geom_point( size = 3) +
    scale_x_continuous(breaks = seq(2008,2018, by=1),
                       limits = c(2008,2020)) +
    scale_y_continuous(breaks = seq(5,30, by =5),
                       limits = c(5,30)) +
    scale_color_manual(values = c(rep( "#008393", 5))) + 
    theme_classic()

youth_plot
```

Now let's add some labels. We can add a title using the `title` argument and a y-axis title using the `y` argument of the `labs()` function.
```{r}
youth_plot <- 
  youth_plot + 
  labs(title = "YOUTH DISCONNECTION BY RACE AND ETHNICITY, 2008 - 2017",
           y = "YOUTH DISCONNECTION (%)") +
  theme(title = element_text(size = 10,
                             color = "#008393",
                             face = "bold"),
        axis.title.x = element_blank())

youth_plot <- 
  directlabels::direct.label(youth_plot, 
                             list(dl.trans(x = x+1.2, y = y + 0), 
                                  "last.points"))
youth_plot
```

Getting close!

Now we need to add figurine icons to the plot.  

To add the figurines we can use icons from free resources like [font awesome](https://fontawesome.com/). See [here](https://commons.wikimedia.org/wiki/Category:Font_Awesome_4_icons) for a link to `svg` options. Or we could also take a screenshot of just the figurine from the original plot. However, it is useful to know how to create similar icons if we didn't have the original plot to work with. 

AVOCADO: i didn't quite understand what you are doing with `image_fill`? could you provide more text on how this works? 

##### Using font awesome:
```{r, out.width="10%"}

fa_figurine <- image_read("https://upload.wikimedia.org/wikipedia/commons/7/7c/User_font_awesome.svg")

fa_figurine

fa_figurine <- image_fill(fa_figurine,
                          color = "#008393", 
                          point = "+800+800",
                          fuzz = 0)
fa_figurine
fa_figurine <- image_fill(fa_figurine,
                          color = "#008393", 
                          point = "+800+1000",
                          fuzz = 0)

fa_figurine
```

#### Using a screenshot:
```{r, out.width= "10%"}

figurine <-image_read(here::here("img", "figurine.png"))
figurine
```

Now to add the image of the figurine to the plot we can use the `draw_image()` function of the `cowplot` package. We simply need to specify the name of the image, and then where we would like it to go according to the x- and y-axis and finally the scaled size of the image. This `scale` argument takes a bit of trial and error. 

Here we will add both images of the figurines to a couple of groups:

```{r}
youth_plot +
  cowplot::draw_image(figurine, x = 2017, y = 23.5, scale = 4) +
  cowplot::draw_image(fa_figurine, x = 2017, y = 17.5, scale = 2)
```

Nice!

Now we just need to add this for all of the groups and we also will want to replace our `Native American` label with a two line version so that we look as similar to the report plot as possible.

#### {.think_question_block}
<b><u> Question Opportunity </u></b>

How might we change the Native American label to have two lines? (hint: think about how we separated the rows of our tables)

####

<details> <summary>Click here to reveal the code.</summary>

```{r}
youth_plot <- major_groups_long %>%
   filter(Gender == "All",
          Group != "United States") %>%
   mutate(Race_Ethnicity = str_replace(string = Race_Ethnicity, 
                                      pattern = "Native American",
                                  replacement = "Native\nAmerican"))  %>% 
  ggplot(aes(x = Year, y = Percent, color = Race_Ethnicity)) +
    geom_line( size = 0.5) +
    geom_point( size = 3) +
    scale_x_continuous(breaks = seq(2008,2018, by=1),
                      limits = c(2008,2019)) +
    scale_y_continuous(breaks = seq(5,30, by =5),
                      limits = c(5,30)) +
    theme_classic() +
    labs(title = "YOUTH DISCONNECTION BY RACE AND ETHNICITY, 2008 - 2017",
         y = "YOUTH DISCONNECTION (%)") +
    theme(title = element_text(size = 10,
                              color = "#008393",
                              face = "bold"),
          axis.title.x = element_blank())+
    scale_color_manual(values = c(rep( "#008393", 5)))+
    draw_image(figurine, x = 2017.25, y = 23.5, scale = 4) +
    draw_image(figurine, x = 2017.25, y = 17.5, scale = 4) +
    draw_image(figurine, x = 2017.25, y = 13.5, scale = 4) +
    draw_image(figurine, x = 2017.25, y = 9, scale = 4) +
    draw_image(figurine, x = 2017.25, y = 5.5, scale = 4)

youth_plot <- directlabels::direct.label(youth_plot, list(dl.trans(x = x + 1.4, 
                                                                   y = y + 0),  
                                                               "last.points"))
```

</details>

```{r}
youth_plot
```
Nice! Our plot looks very similar stylistically to the plot in the report. 

We can see from this plot that Native Americans have a very high rate of youth disconnection, with roughly still a quarter of the population experiencing youth disconnection in this survey. Although some groups show a downward trend, like the Latinx group, the level has been fairly stable since 2008, as we see that the slope of each line is fairly flat. We will investigate this further in our analysis. 

### Major Race and Ethnic Groups and Gender Plot
***
Now let's create a similar plot for females and males. To do this we can simply use the `facet_wrap()` function. We need to specify what variable we want to facet by, in this case the `Gender` variable, by using the `~` operator. We also want to set the `scales` argument to `"free"`, so that we have a y-axis for both the female and male plots. See this [case study](https://opencasestudies.github.io/ocs-bp-co2-emissions/#Faceted_plots) for more information about faceting plots.

We do not want to plot the `"All"` values for the `Gender` variable as we have already shown that in the previous plot, so we again exclude it. 

#### {.recall_code_question_block}
<b><u> Question Opportunity </u></b>

How do we modify the plot above to not include the `"All"` values for the `Gender` variable and stratify by `Gender`?

####

<details> <summary>Click here to reveal the code.</summary>

```{r}
plot_by_gender <- major_groups_long %>%
   filter(Race_Ethnicity != "All_races", Gender != "All") %>%
   mutate(Race_Ethnicity = str_replace(string = Race_Ethnicity, 
                                      pattern = "Native American",
                                  replacement = "Native\nAmerican"),
                  Gender = str_replace(string = Gender, 
                                      pattern = "Female",
                                  replacement = "FEMALE"),
                  Gender = str_replace(string = Gender, 
                                      pattern = "Male",
                                  replacement = "MALE")) %>%
  ggplot(aes(x = Year, y = Percent, color = Race_Ethnicity)) +
    geom_line(size = 0.5) +
    geom_point(size = 3) +
    facet_wrap(~Gender, scales = "free") +
    scale_x_continuous(breaks = seq(2008,2018, by=1),
                       limits = c(2008,2020)) +
    scale_y_continuous(breaks = seq(5,30, by =5),
                       limits = c(5,31)) +
    theme_classic() +
    labs(title = "YOUTH DISCONNECTION BY GENDER AND RACIAL/ETHNIC GROUP",
         y = "YOUTH DISCONNECTION (%)") +
    theme(title = element_text(size = 10,
                               color = "#008393",
                               face = "bold"),
          axis.title.x = element_blank(),
          axis.text.x = element_text(angle = 90),
          strip.background =element_rect(fill="#003661"),
          strip.text = element_text(colour = 'white', face = "bold", 
                                    size = 12)) +
  scale_color_manual(values = c(rep( "#008393", 6)))
 
plot_by_gender <- direct.label(plot_by_gender, list(dl.trans(x = x+0.3, y = y +0), "last.points"))

```
</details>

```{r}
plot_by_gender
```


It's a bit difficult to tell the Black female and Latinx female lines apart. 

Let's try adding another color for every other line. Again we will use the `scale_color_manual()` function to specify the colors of the lines and points on the plot.  

We will use black as the other color. We can use the [hex triplet](https://en.wikipedia.org/wiki/Web_colors?oldformat=true) code or we can write out the name of the color, in this case we will write `"black"`. 
To specifically color each group differently, we can write the colors in the alphabetically order of the distinct values of `Race_Ethnicity`. Thus the first color listed will be the color for the `Asian` values, while the last will be the color for the `White` values.


```{r}
plot_by_gender <- 
  plot_by_gender +
  scale_color_manual(values = c("#008393","black", 
                                "#008393", "#008393",
                                "black"))
```


#### {.explain_block}
<b><u> Question Opportunity </u></b>

Can you explain why we didn't use a pipe in the previous code chunk?

####

```{r}
plot_by_gender
```

Very interesting! By parsing the groups further into intersections of racial and ethnic groups with gender, we see that there are some very striking differences for the Black and Latinx groups when stratified by `Gender`. One positive thing that we can see, is that there has been a steep decline in youth disconnection for Latinx females. Unfortunately, the level of disconnection for females in 2008 was quite high, so now Latinx females have similar rates to Latinx males. In contrast, we see that Black males have had and continue to have much higher rates of disconnection than Black females. Both Black females and males show an increased rate since 2016. Native American females also show an increased rate since 2016. 


### Subgroup plots
***
It is clear from the previous plot that observing more specific subgroups in our data can be very informative!

Thus, we also want to make plots of the Asian and Latinx subgroups, to see if there are particular ethnic groups that have higher levels of youth disconnection. If so, these groups may particularly benefit for prevention and re-engagement efforts.

We will also attempt to continue to plot the genders separately, as we have learned that there may be important gender group differences among the racial and ethnic groups. However, the data is incomplete for some of the ethnic groups. Recall that we also only have two years of data for both our Asian and Latinx subgroup data. First we will start by plotting just the subgroups over the two years.

We can continue to make our plots look like they match the report by using a color palette based off the color used in the report. We can use the `colorRampPalette()` function of the `grDevices` package, which is loaded automatically in an RStudio session. We can specify that we want the colors to range from gray to the color that we have been using.

Then we just need to specify how many colors we want in our gradient when we choose to use the color palette. Say we wanted 4 colors, we just need to type `custom_pal(4)` to get a list of 4 colors within the palette. Thus we will have gray, the teal color we have been using, and two shades in between. After doing this we chose to use one of the colors in between and to use just two colors for our palette.

```{r}
custom_pal <- colorRampPalette(c("gray", "#008393"))
custom_pal <- custom_pal(4)
custom_pal
custom_pal <- c("#008393","#BEBEBE")
```

Here we can see the [hex triplet](https://en.wikipedia.org/wiki/Web_colors?oldformat=true) codes for gray and the shade in between.

OK, now let's start with the Asian subgroups. Since we only have two time points of data for each group, we will create a bar graph this time where the two years are displayed next to each other for each year. Thus, we want the `Year` variable to be a discrete variable. To ensure that it is interpreted as a discrete variable, we need to convert it to be a character type variable rather than numeric using the base `as.character()` function. 


First, we want to filter for the rows where the `Gender` variable values are `All`, we also do not want to include the data for the average of all races. We also 

AVOCADO: the last sentence above seemed like an unfinished thought?

```{r}
asian_subgroups %>%
  filter(Gender == "All") %>%
  filter(Race_Ethnicity != "All_races") %>%
  mutate(Year = as.character(Year))
```

Now that we have wrangled the data we can make our plot. To make a bar plot there are two main `ggplot2` functions,  `geom_col()` and `geom_bar()`. The `geom_col()` function plots the actual values of the data, while the `geom_bar()`function plots counts (however you can override this with the `stat = identity` argument). We are interested in plotting the actual values, so we will use the `goem_col()` function.

This time we will specify that the `Year` variable be used to specify the fill color of the bars of the bar plot by using `aes(fill = Year)`.

We also need to indicate how the bars should be plotted based on the `position` argument.

The options for the `position` argument are:

- `stack` - the years would be displayed on top of one another (this is default)   
- `dodge` - the years would be displayed next to one another with no space in between   
- `dodge2` - the years would be displayed next to one another with a space in between   
- `fill` - the years would be displayed on top of one other, where the heights of each color would show the relative proportions for each year adding up to 1, thus each bar would have the same height     

We will use the `dodge` option.

For the colors, we can try out various numbers of shades for the palette untill we get shades that we like. In this case, 2 shades out of a gradient of 4 shades from gray to teal looks nice,

```{r}  
asian_subgroups %>%
  filter(Gender == "All", Race_Ethnicity != "All_races") %>%
  mutate(Year = as.character(Year)) %>%
  ggplot(aes(x = Race_Ethnicity, y = Percent)) +
    geom_col(aes(fill = Year), position = "dodge")+
    labs(title = "YOUTH DISCONNECTION BY ASIAN SUBGROUP, 2017-2018",
         y = "YOUTH DISCONNECTION (%)") +
    scale_fill_manual(values = custom_pal) +
    theme_classic() +
    theme(title = element_text(size = 10, color = "#008393",
                               face = "bold"),
          axis.title.x = element_blank(),
          axis.text.x = element_text(angle = 90, hjust = 1))
```

OK, this looks pretty nice! However, we can improve this a bit.

Before we do so, let's create a theme for our future similar plots like so:

```{r}
bar_theme <- function() {
  theme_classic() +
  theme(title = element_text(size = 10,
                             color = "#008393",
                             face = "bold"),
        axis.title.x = element_blank(),
        axis.text.x = element_text(angle = 90, hjust = 1))
  }
```

Now we simply need to type `bar_theme()` instead to achieve the same style for our plot.

It would be nice if for subgroups that only have one year of data, if the column that was displayed was still the same width as the that of the other groups. To do this we need a row with an `NA` value for the year that is missing. One way to accomplish this is to use the `pivot_wider()` and `pivot_longer()` functions to widen the data based on the `Year` variable and the collapse the data based on the `Year` variable. By widening our data, we create the `NA` values we want; however we need to collapse our data back to the long format so that we can easily use `ggplot2` to plot the year as the fill in our bar plot.

Note that we no longer need to change the type of the `Year` variable, as it is automatically converted to type character.

```{r}
asian_subgroups %>%
  filter(Gender == "All", Race_Ethnicity != "All_races") %>%
  dplyr::select(-Group) %>%
  pivot_wider(names_from = Year, values_from = Percent)
```

Great! Now we have `NA` values. Now we just to get the data back into long format:

```{r}
asian_subgroups %>%
  filter(Gender == "All", Race_Ethnicity != "All_races") %>%
  dplyr::select(-Group) %>%
  pivot_wider(names_from = Year, values_from = Percent) %>%
  pivot_longer(cols = -c(Race_Ethnicity, Gender), 
               names_to ="Year" , 
               values_to="Percent")
```

Great, now let's see how this looks in the plot.

```{r}  
asian_subgroups %>%
  filter(Gender == "All", Race_Ethnicity != "All_races") %>%
  dplyr::select(-Group) %>%
  pivot_wider(names_from = Year, values_from = Percent) %>%
  pivot_longer(cols = -c(Race_Ethnicity, Gender), 
               names_to ="Year" , 
               values_to="Percent") %>%
  ggplot(aes(x = Race_Ethnicity, y = Percent)) +
    geom_col(aes(fill = Year), position = "dodge")+
    labs(title = "YOUTH DISCONNECTION BY ASIAN SUBGROUP, 2017-2018",
         y = "YOUTH DISCONNECTION (%)") +
    scale_fill_manual(values = custom_pal) +
    bar_theme()
```

Great! that is looking better! However, currently the subgroups are plotted on the x-axis by alphabetically order and we want to instead order the subgroups based on the percentage of youth disconnection. We can use the `forcats` package to do this. 

The `fct_reorder()` function can be used to order the `Race_Ethnicity` variable based on the average of the `Percent` variable for the two years, while the `fct_relevel()` function can be used to make the `Asian` level appear first for comparison sake. Importantly, we need to do this before we reshape the data, because subgroups with `NA` values will be placed at the end.

```{r}
asian_subgroups %>%
  filter(Gender == "All", Race_Ethnicity != "All_races") %>%
  mutate(Race_Ethnicity = fct_reorder(Race_Ethnicity, Percent), 
         Race_Ethnicity = fct_relevel(Race_Ethnicity, "Asian")) %>% 
  select(-Group) %>%
  pivot_wider(names_from = Year, values_from = Percent) %>%
  pivot_longer(cols = -c(Race_Ethnicity, Gender), 
               names_to ="Year", values_to="Percent")%>%
  ggplot(aes(x = Race_Ethnicity, y = Percent)) +
    geom_col(aes(fill = Year), position = "dodge")+
    labs(title = "YOUTH DISCONNECTION BY ASIAN SUBGROUP",
         subtitle = "ORDERED BY AVERAGE DISCONNECTION LEVEL OF 2017 & 2018",
         y = "YOUTH DISCONNECTION (%)") +
    scale_fill_manual(values = custom_pal) +
    bar_theme()
```
This is looking very good! Now, let's make a gap between the Asian average group and the subgroups to more easily differentiate between the two and let's use that space to add the figure legend to the plot itself.

To first make a gap, we can use the `scale_x_discrete()` function. We need to write the name of the levels of the `Race_Ethnicity` variable and place a blank in between the `Asian` level and the subsequent levels. We can use the `levels()` function to make this more reproducible and to avoid mistakes instead of actually writing out each of the levels by hand.

Here you can see all the levels:

```{r}
asian_subgroups %>%
  dplyr::select(-Group) %>%
  filter(Gender == "All", Race_Ethnicity != "All_races") %>%
  mutate(Race_Ethnicity =  fct_reorder(Race_Ethnicity, Percent),
         Race_Ethnicity = fct_relevel(Race_Ethnicity, "Asian")) %>%
  pivot_wider(names_from = Year, values_from = Percent) %>%
  pivot_longer(cols = -c(Race_Ethnicity, Gender), 
               names_to = "Year", values_to = "Percent")%>%
  pull(Race_Ethnicity) %>%
  levels()
```
To simplify our code we will save our wrangled data as a tibble called `df_asian_subgroup`.

So we would like our x-axis to be like this:
```{r, eval = FALSE}
scale_x_discrete(
  limits = c("Asian",
             "Blank",
             "NEXT_blank",
             levels(pull(df_asian_subgroup, Race_Ethnicity))[2:10]),
  labels = c("Blank" = "",
             "NEXT_blank"= ""))
```

The `levels(pull(df_asian_subgroup, Race_Ethnicity))[2:10])` code gets all 9 other levels but the `Asian` level of the `Race_Ethnicity` variable. The `labels` argument specifies what the x-axis should say for the spaces in between. We would like them to say nothing, so we use nothing within quotes `""` as the label. 

We can also use this as an opportunity to change the label on the plot for the `Asian` level of the `Race_Ethnicity` variable to be `"Asian avg."`. We don't want to change the other subgroup level labels so we will not list anything.

We can make this more reproducible by using the `length()` function instead of the number 10 and replacing `Asian` with the first level:

```{r, eval = FALSE}
scale_x_discrete(
  limits = c(levels(pull(df_asian_subgroup, Race_Ethnicity))[1],
             "Blank",
             "NEXT_blank",
             levels(pull(df_asian_subgroup, Race_Ethnicity))[2:
             length(levels(pull(df_asian_subgroup, Race_Ethnicity)))]),
  labels = c("Asian" = "Asian Avg.",
             "Blank" = "",
             "NEXT_blank"= ""))
```

Then we can use `legend.justification` and `legend.position` of the `theme()` function to move our legend to the plot area. 

According to the documentation for the `theme()` function of the `ggplot2` package, the `legend.justification` argument specifies the:

> anchor point for positioning legend inside plot ("center" or two-element numeric vector) or the justification according to the plot area when positioned outside the plot

By setting the mapping for the positioning and justification as `c(0.12,0.1)` we specify that we want the legend to be 12 percent of the plot area from the y-axis and 10 percent of the plot area from the x-axis.

```{r}
df_asian_subgroup <-
  asian_subgroups %>%
  dplyr::select(-Group) %>%
  filter(Gender == "All", 
         Race_Ethnicity != "All_races") %>%
  mutate(Race_Ethnicity =  fct_reorder(Race_Ethnicity, Percent), 
         Race_Ethnicity = fct_relevel(Race_Ethnicity, "Asian")) %>%
  pivot_wider(names_from = Year, values_from = Percent) %>%
  pivot_longer(cols = -c(Race_Ethnicity, Gender), 
               names_to = "Year", values_to = "Percent")

asian_subgroup_plot <-
  df_asian_subgroup %>%
  ggplot(aes(x = Race_Ethnicity, y = Percent, fill = Year)) +
    geom_col(aes(fill = Year), position = "dodge")+
    labs(title = "YOUTH DISCONNECTION BY ASIAN SUBGROUP",
         subtitle = "ORDERED BY AVERAGE DISCONNECTION LEVEL OF 2017 & 2018",
         y = "YOUTH DISCONNECTION (%)") +
  scale_fill_manual(values = custom_pal) +
  scale_x_discrete(
    limits = c(levels(pull(df_asian_subgroup,Race_Ethnicity))[1],
             "Blank",
             "NEXT_blank",
             levels(pull(df_asian_subgroup,Race_Ethnicity))[2:
             length(levels(pull(df_asian_subgroup,Race_Ethnicity)))]),
    labels = c("Asian" = "Asian Avg.", "Blank" = "", "NEXT_blank"= "")) +
  scale_y_continuous(limits = c(0,max(pull(asian_subgroups,Percent), 
                                       na.rm = TRUE)))+
  bar_theme() +
  # this add the legend to the plot area!:
        theme(legend.justification = c(0.12,0.1),
              legend.position = c(0.12,.1))

asian_subgroup_plot
```

Nice! But, this is still missing something. Perhaps if we add lines that show the average disconnection rate for the US and for Asians in general, then it will make it easier to see differences in our plot.

In the previous code we saved our plot to a object called `asian_subgroup_plot`.

We can modify it by adding layers to add the lines that we want.

First, we want to create tibbles for the US Asian average disconnection rates. We want to add labels to each of these. We can use the `parse = TRUE` argument later with the `geom_text()` function of the `ggplot2` package to make these labels appear as bold font as it will be evaluated as an expression instead of a simple character string.

```{r}
df_asian_total <- 
  asian_subgroups %>%
  filter(Year == 2017, Gender == "All", Race_Ethnicity == "Asian") %>%
  mutate(Year = as.character(Year), 
         label = c('bold("ASIAN 2017 AVG RATE")'))

df_US_total <- 
  asian_subgroups %>%
  filter(Year == 2017, Gender == "All", Race_Ethnicity == "All_races") %>%
  mutate(Year = as.character(Year), label = c('bold("US 2017 AVG RATE")'))

df_asian_total
df_US_total
```

Now we can use these to create labels again with the `geom_text()` function and lines with the `geom_hline()` function on our plot. 

The `geom_text()` function requires that the x and y axis location for the text be specified, and we want this to be located near the actual percent disconnection rate, thus we use the `Percent +.5` to grab the `Percent` value from `df_asian_total` and add 0.5 to it to be slightly above where the line will be. 

The `geom_hline()` function requires, just the y intercept as this function always creates a horizontal line. The `linetype` argument specifies what style of line we would like. See [here](http://sape.inf.usi.ch/quick-reference/ggplot2/linetype) for a list of options.

```{r}
asian_subgroup_plot <- 
  asian_subgroup_plot + 
  geom_text(data = df_asian_total, 
            mapping = aes(x = 2.5, y = Percent +.7, label = label),
            parse = TRUE,
            color = "#008393", 
            size = 4) +
  geom_hline(aes(yintercept = pull(df_asian_total, Percent)), linetype = 2) +
  geom_text(data = df_US_total,
            mapping = aes(x = 2.5, y = Percent +.7, label = label),
            parse = TRUE,
            color = "#008393", 
            size = 4) +
  geom_hline(aes(yintercept = pull(df_US_total, Percent)), linetype = 3)

asian_subgroup_plot
```

Awesome! Now it is much easier to tell how the disconnection rates for various subgroups compare to the national average and the Asian average. From our plot, we can see that the [Hmong](https://en.wikipedia.org/wiki/Hmong_people) and Cambodian subgroup rates are very high. If we only had the plot of major racial and ethnic groups to rely on, we might not realize that these two groups have such high rates. 

Nice! This shows the importance of adding small details such as the US and Asian rate lines. This helps provide a simple yet nuanced picture of what is going on. 

From the above plot, it becomes readily apparent that the Hmong and Cambodian subgroups have much higher disconnection rates than the other Asian subgroups, as well as all races / ethnicities in the US combined. 

If we wanted to make a similar plot but without the subgroups that are missing a year we could do it like so, by adding the `drop_na()` function of the `tidyr` package to remove rows with `NA` values when the data  is in wide  form and then using the `fct_drop()` function of the `forcats` package to remove the levels of the `Race_Ethnicity` variable that no longer have rows in the data. 

<details> <summary> Click here to reveal the code. </summary> 
```{r}
df_asian_subgroup <-
  asian_subgroups %>%
  dplyr::select(-Group) %>%
  filter(Gender == "All", Race_Ethnicity != "All_races") %>%
  mutate(Race_Ethnicity =  fct_reorder(Race_Ethnicity, Percent), 
         Race_Ethnicity = fct_relevel(Race_Ethnicity, "Asian")) %>%
  pivot_wider(names_from = Year, values_from = Percent) %>%
  tidyr::drop_na() %>%
  pivot_longer(cols = -c(Race_Ethnicity, Gender), 
               names_to = "Year", values_to = "Percent") %>%
  mutate(Race_Ethnicity = fct_drop(Race_Ethnicity))

asian_subgroup_plot_simp <-
  df_asian_subgroup %>%
  ggplot(aes(x = Race_Ethnicity, y = Percent, fill = Year)) +
    geom_col(aes(fill = Year), position = "dodge")+
    labs(title = "YOUTH DISCONNECTION BY ASIAN SUBGROUP",
         subtitle = "ORDERED BY AVERAGE DISCONNECTION LEVEL OF 2017 & 2018",
         y = "YOUTH DISCONNECTION (%)") +
    scale_fill_manual(values = custom_pal) +
    scale_x_discrete(
    limits = c(levels(pull(df_asian_subgroup,Race_Ethnicity))[1],
               "Blank",
               "NEXT_blank",
               levels(pull(df_asian_subgroup,Race_Ethnicity))[2:
               length(levels(pull(df_asian_subgroup,Race_Ethnicity)))]),
    labels = c("Asian" = "Asian Avg.",
               "Blank" = "",
               "NEXT_blank"= ""))+
    scale_y_continuous(limits = c(0,max(pull(asian_subgroups,Percent), 
                                        na.rm = TRUE)))+
    bar_theme() +
    #this add the legend to the plot area!:
    theme(legend.justification = c(0.16,0.01),legend.position = c(0.16,.01)) +
    geom_text(data = df_asian_total,
              mapping = aes(x = 2.5, y = Percent +.7, label = label),
              parse = TRUE, color = "#008393", size = 4) +
    geom_hline(aes(yintercept = pull(df_asian_total, Percent)), linetype = 2) +
    geom_text(data = df_US_total,
              mapping = aes(x = 2.5, y = Percent +.7, label = label),
              parse = TRUE, color = "#008393", size = 4) +
    geom_hline(aes(yintercept = pull(df_US_total, Percent)), linetype = 3)
```
</details> 

```{r}
asian_subgroup_plot_simp
```

***

Now let's do the same for the Latinx subgroups.

First let's make the small tibble of the 2017 Latinx average rate like we did for the US and for Asians to create the lines and text on our plot.

```{r}
df_latinx_total <- 
  latinx_subgroups %>%
  filter(Year == 2017, Gender == "All", Race_Ethnicity == "Latinx") %>%
  mutate(Year = as.character(Year), 
         label = c('bold("LATINX 2017 AVG RATE")'))
```


Now we will use similar code for the latinx subgroups. The only difference is that we will also change the `"Puerto Rican, Dominican, Cuban"` values to have new line breaks between the groups. First let's plot all subgroups.

<details> <summary> Click here to reveal the code. </summary> 
```{r}
latinx_for_plot <-latinx_subgroups %>%
  dplyr::select(-Group) %>%
  filter(Gender == "All", Race_Ethnicity != "All_races") %>%
  mutate(Race_Ethnicity = str_replace(
                 string = Race_Ethnicity, 
                pattern = "Puerto Rican, Dominican, Cuban",
            replacement = "Puerto Rican,\nDominican,\nCuban")) %>%
  mutate(Race_Ethnicity = fct_reorder(Race_Ethnicity, Percent), 
         Race_Ethnicity = fct_relevel(Race_Ethnicity, "Latinx")) %>%
  pivot_wider(names_from = Year, values_from = Percent) %>%
  pivot_longer(cols = -c(Race_Ethnicity, Gender), 
  names_to = "Year" , 
  values_to = "Percent")

latinx_subgroup_plot <- 
  latinx_for_plot %>%
  ggplot(aes(x = Race_Ethnicity, y = Percent, fill = Year)) +
    geom_col(aes(fill = Year), position = "dodge") +
    labs(title = "YOUTH DISCONNECTION BY LATINX SUBGROUP",
         subtitle = "ORDERED BY AVERAGE DISCONNECTION LEVEL OF 2017 & 2018",
         y = "YOUTH DISCONNECTION (%)") +
    scale_fill_manual(values = custom_pal) +
    scale_x_discrete(
    limits = c(levels(pull(latinx_for_plot,Race_Ethnicity))[1],
               "Blank",
               "NEXT_blank",
               levels(pull(latinx_for_plot,Race_Ethnicity))[2:
               length(levels(pull(latinx_for_plot,Race_Ethnicity)))]),
    labels = c("Latinx" = "Latinx Avg.",
               "Blank" = "",
               "NEXT_blank"= ""))+
    bar_theme() +
    #this add the legend to the plot area!:
    theme(legend.justification = c(0.23,0.1),legend.position = c(0.23,.1))+
    geom_text(data = df_latinx_total,
           mapping = aes(x = 3, y = Percent +.7, label = label),
             parse = TRUE, color = "#008393", size = 4) +
    geom_hline(aes(yintercept = pull(df_latinx_total, Percent)), linetype = 2) +
    geom_text(data = df_US_total,
           mapping = aes(x = 3, y = Percent +.7, label = label),
             parse = TRUE, color = "#008393", size = 4) +
    geom_hline(aes(yintercept = pull(df_US_total, Percent)), linetype = 3)
```
</details> 

```{r}
latinx_subgroup_plot
```

Now let's drop the subgroups that don't have values for both years.

<details> <summary> Click here to reveal the code. </summary> 

```{r}
latinx_for_plot <-latinx_subgroups %>%
  dplyr::select(-Group) %>%
  filter(Gender == "All") %>%
  filter(Race_Ethnicity != "All_races") %>%
  mutate(Race_Ethnicity = str_replace(
                 string = Race_Ethnicity, 
                pattern = "Puerto Rican, Dominican, Cuban",
            replacement = "Puerto Rican,\nDominican,\nCuban")) %>%
  mutate(Race_Ethnicity =  fct_reorder(Race_Ethnicity, Percent)) %>% 
  mutate(Race_Ethnicity = fct_relevel(Race_Ethnicity, "Latinx")) %>%
  pivot_wider(names_from = Year, values_from = Percent) %>%
  tidyr::drop_na() %>%
  pivot_longer(cols = -c(Race_Ethnicity, Gender), 
  names_to = "Year" , 
  values_to = "Percent")%>%
  mutate(Race_Ethnicity = fct_drop(Race_Ethnicity))

latinx_subgroup_plot_simp <-latinx_for_plot %>%
ggplot(aes(x = Race_Ethnicity, y = Percent, fill = Year)) +
 geom_col(aes(fill = Year), position = "dodge")+
  labs(title = "YOUTH DISCONNECTION BY LATINX SUBGROUP",
    subtitle = "ORDERED BY AVERAGE DISCONNECTION LEVEL OF 2017 & 2018",
           y = "YOUTH DISCONNECTION (%)") +
  scale_fill_manual(values = custom_pal) +
  scale_x_discrete(
  limits = c(levels(pull(latinx_for_plot,Race_Ethnicity))[1],
             "Blank",
             "NEXT_blank",
             levels(pull(latinx_for_plot,Race_Ethnicity))[2:
             length(levels(pull(latinx_for_plot,Race_Ethnicity)))]),
  labels = c("Latinx" = "Latinx Avg.",
             "Blank" = "",
             "NEXT_blank"= ""))+
  scale_y_continuous(limits = c(0,max(pull(latinx_subgroups,Percent), 
                                       na.rm = TRUE)))+
  bar_theme()+
  #this add the legend to the plot area!:
  theme(legend.justification = c(0.23,0.01),legend.position = c(0.23,.01))+
  geom_text(data = df_latinx_total,
         mapping = aes(x = 3, y = Percent +.7, label = label),
           parse = TRUE,
           color = "#008393", 
            size = 4) +
  geom_hline(aes(yintercept = pull(df_latinx_total, Percent)), linetype = 2) +
  geom_text(data = df_US_total,
         mapping = aes(x = 3, y = Percent +.7, label = label),
           parse = TRUE,
           color = "#008393", 
            size = 4) +
  geom_hline(aes(yintercept = pull(df_US_total, Percent)), linetype = 3)
```

</details> 

```{r}

latinx_subgroup_plot_simp
```

AVOCADO: I think we should discuss whether or not to keep the subgroup analysis in the case study. My concern is (1) most the "subgroup plots" section (both asian and latinx) gets realllllly into the weeds with ggplot2. I'm not sure how relevant this is. (2) the data analysis section doesn't look at this at all, so it makes it feel more like "fluff". And I hate for people to get so bogged down into something we aren't going to use in the next section (at least for educational purposes).

AVOCADO: I moved the "subgroup by gender plots" and the "faceted year and gender plots" to the homework section. 



## **Data Analysis**
*** 

```{r, echo = FALSE}
# To allow starting at this section:
load(here::here("data", "wrangled_data.rda"))
```


In our data, we have pooled (repeated) [cross-sectional](https://en.wikipedia.org/wiki/Cross-sectional_data?oldformat=true) data.

This is data produced from repeated measurement of a [population](https://en.wikipedia.org/wiki/Population?oldformat=true) over time.

It is often unfeasible to collect data for an entire population at once. However, we can still obtain meaningful measures using a random [sample](https://en.wikipedia.org/wiki/Sampling_(statistics)?oldformat=true) of the population. 

At specific time-points, data is collected from a sample of the population. The individuals in each sample are not necessarily the same individuals. This separates pooled cross-sectional data from panel data, which is longitudinal data from repeated measurement of the same people.   

By sampling from a population at multiple time points, we can generate estimates of population level statistics. Although these statistics have some random error, they can provide insight into how the measure variable is changing in a population over time.


As we have measured values over time, we can plot the data to see if there is a trend (increasing, decreasing, monotonic, non-monotonic, linear, non-linear, etc). Sometimes, however, the trend isn't exactly clear. Fortunately, there are statistical methods to resolve this issue.

Next, we will introduce and discuss the Mann-Kendall trend test. 

### Mann-Kendall trend test
***
The Mann-Kendall trend test, (which is a variation of the [Kendall rank correlation coefficient](https://en.wikipedia.org/wiki/Kendall_rank_correlation_coefficient?oldformat=true)), tests whether there is a [monotonic association](https://www.statisticshowto.com/monotonic-relationship/) between two variables, or a relationship with consistent direction between two variables, where one variable (typically time) determines the order of the other variable. Thus this test is most often used to see if there is a consistent change in direction (or [mathematical sign](https://en.wikipedia.org/wiki/Sign_(mathematics))) for a variable across time. 

#### Monotonic relationships
***
As you can see in the image below, a monotonic relationship does not need to be linear, it simply needs to show a consistent direction of change. A linear relationship on the other hand shows consistent direction and rate of change.

```{r, out.width="400px", echo=FALSE}
knitr::include_graphics("https://www.statisticshowto.com/wp-content/uploads/2017/03/monotonic-relationship.png")
```


##### [[source](https://www.statisticshowto.com/monotonic-relationship/) - Stephanie Glen. "Monotonic Relationship: Definition" From StatisticsHowTo.com: Elementary Statistics for the rest of us! https://www.statisticshowto.com/monotonic-relationship/]


So in other words, we are interested to see if there is a consistent direction in the relationship of disconnection rates with time, as we want to know if rates have been consistently increasing or decreasing or if they have been staying roughly the same. 

**Note**: this test does not test if the rate of change has been consistent over time, just the direction.

#### Mann-Kendall trend test vs simple linear regression
***

In contrast to the Mann-Kendall trend test, we briefly compare this to [Simple linear regression](https://en.wikipedia.org/wiki/Simple_linear_regression). 

There are a couple of difference between the two: 

1. Linear regression assesses if there a consistent change in both rate and direction in the dependent variable as a single independent variable changes. The coefficients tell us how much the dependent variable changes with one unit change of the independent variable. 

2. The Mann-Kendall trend test is a [nonparametric](https://en.wikipedia.org/wiki/Nonparametric_statistics) test, which means that it does not require as many assumptions as some [parametric](https://en.wikipedia.org/wiki/Parametric_statistics) tests, like the [simple linear regression](https://en.wikipedia.org/wiki/Simple_linear_regression).


### M-K test by hand
***
The [null hypothesis](https://en.wikipedia.org/wiki/Null_hypothesis) $H_{0}$ of the test is that there is no monotonic trend, while the [alternative hypothesis](https://en.wikipedia.org/wiki/Alternative_hypothesis) $H_{a}$ is that there is a monotonic trend.

According to [this report](https://www.statisticshowto.com/wp-content/uploads/2016/08/Mann-Kendall-Analysis-1.pdf), the Mann-Kendall $S$ score is calculated as follows:

> The data values are evaluated as an ordered time series. Each data value is compared to all subsequent data values. The initial value of the Mann-Kendall statistic, $S$, is assumed to be 0 (e.g., no trend). If a data value from a later time period is higher than a data value from an earlier time period, $S$ is incremented by 1. On the other hand, if the data value from a later time period is lower than a data value sampled earlier, $S$ is decremented by 1. The net result of all such increments and decrements yields the final value of $S$. 


$$S =\sum_{k=1}^{n-1}\sum_{j = k+1}^{n} sign(x_{j} - x_{k}) $$

where we assume there are $n$ time points and observations $x_{j}$ and $x_{k}$ at two time points and:

$$sign(x_{j}-x_{k}) = 1 \text{, if } x_{j}-x_{k}>0$$
 $$sign(x_{j}-x_{k}) = 0 \text{, if } x_{j}-x_{k}=0$$
 $$sign(x_{j}-x_{k}) = -1 \text{, if } x_{j}-x_{k}<0$$
                   
                   
Another way of stating this is all possible cases where a value occurred later in time must be compared with values that occurred earlier in time, giving $n(n–1)/2$ comparisons, like so if there were four time points ($n = 4$):

$$S = sign(x_2−x_1) + sign(x_3−x_1) + sign(x_4-x_1) + sign(x_3−x_2) + sign(x_4-x_2) + sign(x_4-x_3)$$

which is consistent with $4(4-1)/2 = 4(3)/2 = 12/2 = 6$ comparisons.

Once we have calculated the Mann-Kendall $S$ score, we want to know if the score is meaningfully large or small enough from what we would expect with data with no trend. To determine that, we need to know what the _variance_ is of the Mann-Kendall $S$ score (or how much variability we expect). 

We won't explain the details here, but it turns out the variance of the Mann-Kendall $S$ score (or $\text{var}(S)$) is calculated like so:

$$ \text{var}(S)=\frac{1}{18}(n(n−1)(2n+5)−\sum_{p-1}^{g}t_{p}(t_{p}−1)(2t_{p}+5))$$
where: 

$$g =  \text{number of tied groups}$$
$$t_{p} =  \text{number of observations in the }p^{th}\text{ group for a given tie}$$

For example, if 3 observations in our example were 35, then there would be 1 $g$ tied group and 3 $t_{p}$ observations. 

**Note**: according to this [source](https://www.statisticshowto.com/mann-kendall-trend-test/), a minimum of 4 observations should be used for this test and it is recommended to have 8 or more. 

Putting this all together, the test statistic we are interested in (to know if the score is meaningfully large or small enough from what we would expect with data with no trend) $Z_{MK}$ is calculated as follows:

 if $S > 0$ then $Z_{MK} = \frac{S-1}{\sqrt{\text{var}(S)}}$  
 if $S < 0$ then $Z_{MK} = \frac{S+1}{\sqrt{\text{var}(S)}}$  
 if $S = 0$ then $Z_{MK} = 0$  

This test statistic $Z_{MK}$ is considered a [Z score](https://en.wikipedia.org/wiki/Standard_score) (also known as a standard or standardized score). We can use standard look up tables from there to calculate a p-value. We will walk through an example of how to do this below. 

Let's conduct a Mann-Kendall trend test using our data to see if there are any monotonic trends in youth disconnection across time. 

#### M-K trend test for Native Americans
***

Recall that the youth disconnection rates for Native Americans were some of the highest in the first table we examined. 

```{r, echo = FALSE}
Major_racial_ethnic_groups
```

and the plot we made: 

```{r}
youth_plot
```

Let's consider just observed values for the Native American subgroup. The first value in time was ($x_1$ = time point 1) 24.4, then ($x_2$) 28.8 at the next time point, then ($x_3$) 27.0, then ($x_4$) 26.3, then ($x_5$) 25.8, and finally ($x_6$) 23.9.

If we were to calculate the Mann-Kendall statistic manually we could do the following:

$$  S = sign(x_2−x_1) + sign(x_3-x_1) + sign(x_4-x_1) + sign(x_5-x_1) + sign(x_6-x_1) +$$

$$ sign(x_3-x_2) + sign(x_4-x_2) + sign(x_5-x_2) + sign(x_6-x_2) +$$  

$$sign(x_4-x_3) + sign(x_5-x_3) + sign(x_6-x_3) +$$  

$$sign(x_5-x_4) + sign(x_6-x_4) +$$   

$$sign(x_6-x_5)$$  

Thus to calculate $S$ manually in R we could do like so:

```{r}
x1 = 24.4
x2 = 28.8
x3 = 27
x4 = 26.3
x5 = 25.8
x6 = 23.9

sign(x2-x1) + sign(x3-x1) + sign(x4-x1) + sign(x5-x1) + sign(x6-x1) +
sign(x3-x2) + sign(x4-x2) + sign(x5-x2) + sign(x6-x2) +sign(x4-x3) + sign(x5-x3) + sign(x6-x3) +sign(x5-x4) + sign(x6-x4) + sign(x6-x5)
```

The result is -7. 

The $\text{var}(S)$ would be calculated like so (since we have $n$ = 6 observations and no ties):

$$ \text{var}(S)=\frac{1}{18}(6(6−1)(2(6)+5)−\sum_{p-1}^{g}0(0−1)(2(0)+5))$$
$$ \text{var}(S)=\frac{1}{18}(30)(17)−0 =1.666667(17) = 28.33333$$

Finally, we calculate the standardized score $Z_{MK}$ (with $S$ < 0):

 $Z_{MK} = \frac{S+1}{\sqrt{VAR(S)}}$
 $Z_{MK} = \frac{-7+1}{\sqrt{28.33333}} = \frac{-6}{5.322906} = -1.127$
 
As we noted above, the $Z_{MK}$ is the specific Mann-Kendall version of the [Z score](https://en.wikipedia.org/wiki/Standard_score). This allows us to use a variety of data (which maybe normally distributed or not) and create a standard statistic about the data that is normally distributed. 

Using the $Z_{MK}$ Z score, we can use a standard $Z$ table to determine the $p$-value. 

The $p$-value tells us the probability of observing a test statistic as or more extreme than the one we observed if the null hypothesis (that there is no trend) is true. If the $p$-value is small, say less than 0.05, then our observed $Z_{MK}$ statistic is pretty rare if the null hypothesis is true. This would lead us to reject the null hypothesis and conclude that there is indeed a trend in the data. We will talk more about $p$-values, and in particular our choice of 0.05 as a comparison for the $p$-value, a little later. See this [case study](https://opencasestudies.github.io/ocs-bp-rural-and-urban-obesity/) for more information about probability and hypothesis testing.

Ok back to our example. Let's figure out what our $p$-value is using the standard $Z$ score table.

Using the table below, in the the $Z$ column on the far left, we find a row that says -1.1 and because our $Z$ value is -1.127 we round this to -1.13 and then find the column for 0.03 to account for the value that is second after decimal place for our $Z$ score. We see that the value where this row and this column intersect is 0.1292. This is the $p$-value, that indicates the probability of having a $Z$ score that is more extreme (on the negative side) of the [$Z$ score normal distribution](https://en.wikipedia.org/wiki/Standard_score) or the colored portion of the distribution in the figure below. In other words, there is a 13% chance of observing data with a more negative $Z$ score (and also a downward trend as the sign of the $Z_{MK}$ is determined by the $S$ score) simply by chance alone.
 
```{r,echo=FALSE, out.width="80%"}
 include_graphics(here::here("img", "zscore.png"))
```

##### [[source]](https://socratic.org/questions/5986a3e1b72cff6fd48a5408)

Since we do not know what type of trend we are looking for (thus we are using what is called a [two-sided hypothesis](https://en.wikipedia.org/wiki/One-_and_two-tailed_tests) - with the the alternative hypothesis or $H_{a}$ being that either a positive or negative trend exists), we can use this table to determine that the $p$-value is ~.1292 for a one-sided hypothesis (specifically testing that a negative trend exists or testing that a positive trend exists) and multiply this by 2 to get a $p$-value of approximately ~ 0.26 for a two-sided hypothesis.

See this [article](https://www.onesided.org/articles/the-paradox-of-one-sided-v-two-sided-tests-of-significance.php) for more information about one-sided and two-sided tests.

Thus in other words, there is a 26% chance of observing a more positive or more negative z score by chance alone. This is a pretty high chance!

Thus visualize this, our two-sided test looks for the areas on the probability distribution of the Z score that are more positive or more negative for both signs of the $Z$ value:

```{r, out.width="30%", echo=FALSE}
knitr::include_graphics(here::here("img", "twosided.png"))
```

Alternatively, we can use this [calculator](https://www.socscistatistics.com/pvalues/normaldistribution.aspx), which gives us a more precise $p$-value of 0.26 for the two-sided hypothesis.
 
Either way, our $p$-value of ~ 0.26 suggests that we do not have a monotonic trend as our $p$-value is much larger than the typical [significance threshold](https://en.wikipedia.org/wiki/Statistical_significance) of 0.05. The idea here is that it is acceptable that there is a 5% [probability](https://en.wikipedia.org/wiki/Probability) that our observation occurred simply due to chance. 

In our case there is not enough evidence to reject the null hypothesis. Thus we conclude that there is likely no trend.

### M-K test implementation in R
***
We can also accomplish this with the `MannKendall()` function of the `Kendall` package. This function requires a vector of data for which a trend may be observed. See the [documentation](https://rdrr.io/cran/Kendall/man/MannKendall.html) for this function of the `Kendall` package for more details.

#### M-K trend test for Native Americans
***
In the following we: 

1. Filter for just the data for the Native American group
2. Use the `pull()` function of the `dplyr` package to get just the vector of this data
3. Apply the `MannKendall()` function from the `Kendall` package
4. Use the base `summary()` function to get more information for the results of our Mann-Kendall trend test

```{r}
major_groups_long %>%
  filter(Gender == "All", Race_Ethnicity == "Native American") %>%
  pull(Percent) %>%
  MannKendall(.) %>%
  summary()
```

Again, just like when we calculated this manually, we see that the score $S$ is -7. The larger the score, the more indication that there is an increasing or decreasing monotonic relationship. We also again see that the $p$-value is roughly = 0.26 and is greater than 0.05, suggesting that we failed to find a monotonic trend in the data. 

avocado... need to make sure it is ok that we don't have the exact same number of years in between each time point...

However, it's important to note that we only have `r length(major_groups_long %>% filter(Gender == "All", Race_Ethnicity == "Native American") %>% pull(Percent))` observations. Thus, we may not have enough observations to observe a trend.

We can also explore whether there is a **linear** trend using [simple linear regression](https://en.wikipedia.org/wiki/Simple_linear_regression?oldformat=true) by using the `lm()` function of the `stats` package.

First, we visualize the data with `Year` on the x-axis and `Percent` of youth disconnection on the y-axis.

```{r}
major_groups_long %>%
  filter(Gender == "All", Race_Ethnicity == "Native American") %>%
  ggplot(aes(x = Year, y = Percent)) +
    geom_point() 
```

We are interested in modeling how the percent disconnection changes with time, thus we will use the formula `Percent ~ Year` in the `lm()` function, which uses the `Percent` and `Year` variables. See this [case study](https://opencasestudies.github.io/ocs-bp-diet/) for more information about linear regression. 

```{r}
major_groups_long %>%
  filter(Gender == "All",
  Race_Ethnicity == "Native American") %>%
  lm(Percent ~ Year, data = .) %>%
  summary()
```

Based on the coefficients in the output, for each one year change, the mean increase in disconnection rates is
`r (major_groups_long%>% filter(Gender == "All", Race_Ethnicity == "Native American") %>% lm(Percent ~ Year, data = .))[["coefficients"]]["Year"]`

We can also see from the $p$-value (`Pr(>|t|)`), that this relationship is not statistically significant, as it is much larger than 0.05. Again, we are however largely limited by the number of observations in this dataset. 

We can visualize the relationship above like so using the `geom_smooth()` function of the `ggplot2` package with the `method = "lm"` argument to show the linear model.

```{r}
major_groups_long %>%
  filter(Gender == "All",
  Race_Ethnicity == "Native American") %>%
  ggplot(aes(x = Year, y = Percent)) +
  geom_smooth(method = "lm", color = "red") + 
  geom_point() + 
  scale_x_continuous(breaks = seq(2008, 2018, by = 1),
                     labels = seq(2008, 2018, by = 1),
                     limits = c(2008, 2018)) +
  theme_minimal() +
  labs(title = "Youth Disconnection Rates of Native American Youth",
       subtitle = "2008 - 2017",
       x = "Year",
       y = "Disconnection Rate")
```

As we can see, that although the line appears to have a negative slope, there is a large amount of uncertainty around the fitted line. 

***

#### Female Latinx 
***

What about if we look at data that we think we might see a trend according to our data visualizations. The female Latinx data looks as though it may be decreasing:

```{r, echo = FALSE}
plot_by_gender
```

```{r}
major_groups_long %>%
  filter(Gender == "Female", Race_Ethnicity == "Latinx") %>%
  pull(Percent) %>%
  MannKendall(.) %>%
  summary()
```

Indeed this data does shows a trend of declining rates of youth disconnection as the score is again negative and larger and the $p$-value is now  much less than 0.5 for even the 2-sided hypothesis. 

The Z score can be calculated like so:

 $Z_{MK} = \frac{-13+1}{\sqrt{28.33333}} = \frac{-12}{5.322906} = -2.254408$

The one tailed p-value is half of the 2-sided p-value and is thus roughly 0.012. We just tested two hypotheses, one for the Native Americans and one for the Latinx female population. So we should use the [Bonferroni multiple testing correction](https://en.wikipedia.org/wiki/Bonferroni_correction) to adjust our significance threshold $\alpha$ to be $.05/(n\text{ tests}) = 0.5/2 = .025$. Our $p$-value for the one sided test was 0.012 which is less than .025.  This suggests that indeed there is a decreasing trend for youth disconnection among the Female Latinx population. 

AVOCADO: maybe more info about bonferroni here.

***

## **Summary**
*** 

### Summary Plot

Now we will create a plot that summarize our findings. 


We will create a plot that is just an arrow using `geom_segment()`. 

This requires the following arguments :
1) x -  x coordinate of starting point of the arrow
2) y - y coordinate of starting point of the arrow
3) xend -  x coordinate of end point of the arrow
4) yend - y coordinate of end point if the arrow

Also `ggplot` requires a data input, we will use this to create the y axis value for the arrow. We will use the `aes()` function to define the length and location of the arrow.


We need to use the `arrow` argument with the `arrow()` function to create an arrow. The `lineend` and `linejoin` arguments specify what styles to use. The `size()` function specifies how thick the arrow is displayed.

See [here](https://ggplot2.tidyverse.org/reference/geom_segment.html) for different arrow style options.

Then we will use the `xlim()` function of the `ggplot2` package to specify the overall size of the plot relative to the arrow.


```{r}
arrow_df <- data.frame( y = 1)
arrow_df

arrowplot<-ggplot(arrow_df, aes(x = .5, y = 1, xend = 1, yend = 1)) +
                geom_segment(arrow = arrow(),
                           lineend = "butt",
                          linejoin = "mitre",
                              size = 3)+
                xlim(0.25, 1.25) + ylim(0,1.5)+
  geom_text( label = "Look at\n subgroups", aes(x = .75, y = .8), size = 5, face = "bold")

arrowplot
```


Then we will use `theme_void()` remove  the background.


```{r}
arrowplot <- arrowplot +  theme_void()
arrowplot
```



We will use the `patchwork` package to do so.

This package allows us to create formulas for our plot layouts. Thus we can use `+` to add plots and plot elements like `plot_spacer()` to add an empty space` and we can use `/` to place plots in different rows on top of one another like a fraction. 

Thus we can combine our plots that we previously named and saved as objects.


```{r}

youth_plot + arrowplot  + (plot_by_gender /asian_subgroup_plot_simp) 
```

The `plot_layout()` function allows for specification of the relative heights and widths of the various plot elements. Then, since we will create three columns (distinguished by the `+` symbols, we need to specify widths and heights for all three.)


We can change aspects about  the plots using the `+` symbol and the `theme()` function as we have previously used it.  We will also add an overall title using the `plot_annotation()` function.

Finally, we will use the `png()` function of the `grDevices` package which is automatically loaded in RStudio sessions to save the plot. 
We will use the `here()` function of the `here` package, to specify that we want to save it in the `img` directory and call it `mainplot.png`.
We can also use this function to specify the resolution with `res` and in doing so, we need to save the image with size specifications to make it larger. 

```{r}

plot_by_gender  <-plot_by_gender + theme(axis.text =element_text(size = 12))
asian_subgroup_plot_simp  <-asian_subgroup_plot_simp + theme(axis.text =element_text(size = 12))

png(filename = here::here("img", "mainplot.png"),
    res = 300, width = 15, height = 10, units = "in")
    youth_plot  +
      theme(axis.text =element_text(size = 14, angle = 90)) +
    arrowplot  + 
      (plot_by_gender /
         asian_subgroup_plot_simp)  + 
      plot_layout(widths = c(4.6, 1, 4.65)) &
plot_annotation(title = "Deeper Inspection Reveals Unexpected Differences for Youth Disconnection Among\nGender and Racial/Ethnic Subgroups", 
               theme = theme(plot.title = element_text(size = 22, 
                                                       face = "bold")))

dev.off()
```





```{r}
knitr::include_graphics(here::here("img", "mainplot.png"))
```


### Synopsis

In this case study we evaluated rates of youth disconnection in the United States between 2008 and 2018, which was defined as individuals between age **16 - 24** who are **neither working nor in school**". In particular, we focused on evaluating specific subgroups to identify populations that may experience particularly high levels of disconnection. We used data from two [Measure of America](https://measureofamerica.org/) reports:  

1) The 2019 report called [Making the Connection](https://ssrc-static.s3.amazonaws.com/moa/Making%20the%20Connection.pdf)  
2) The 2020 report called [A Decade Undone](https://ssrc-static.s3.amazonaws.com/moa/ADecadeUndone.pdf)  

The data in these reports comes from the [American Community Survey(ASC)](https://www.census.gov/programs-surveys/acs).

We demonstrated how to import data from these PDFs by extracting the text from screen shot images of the tables were were particularly interested in. 

It was identified in the 2019 [Measure of America annual report](https://ssrc-static.s3.amazonaws.com/moa/Making%20the%20Connection.pdf) that Native American populations particularly experience high rates of youth disconnection. Indeed roughly a quarter of this population appears to experience youth disconnection, which is much higher than the other major racial groups. By performing the [Mann-Kendall trend test](https://cran.r-project.org/web/packages/Kendall/Kendall.pdf)(https://www.statisticshowto.com/mann-kendall-trend-test/) both manually and using the `MannKendall()` function of the `Kendall` package, we demonstrated that the rate does not appear to be changing. This suggests that more prevention and re-engagement strategies should especially focus on this population.

By plotting subgroups within our data, it is clear that there are differences among particular populations. We see that there are much higher levels of disconnection among youths who are male and black as compared to black females. 
We also see that youth disconnection rates among [Hmong](https://en.wikipedia.org/wiki/Hmong_people) and Cambodian populations are very high, despite rates being quite low for the Asian population in general.

Overall, this case study demonstrates the importance of looking at populations at deeper levels. This is something of great importance facing the field of public health. Where possible data should be collected on a variety of demographic groups to better characterize vulnerable populations.

avocado talk about statistical limitations more?

## **Suggested Homework**
*** 

- For the asian and latinx subgroup barplots made across year, modify these plots to consider gender differences (instead of across time).
- Taking the plot you made above, modify the plot to facet across years. 
- Find another table in one of the reports to import using the `magick` package (for example perhaps the data about different states over time in the 2019 report called [Making the Connection](https://ssrc-static.s3.amazonaws.com/moa/Making%20the%20Connection.pdf)) Look for differences between groups by plotting the data and evaluating with the Mann-Kendall test. 

## **Additional Information**
*** 



### Helpful Links

[RStudio](https://rstudio.com/products/rstudio/features/){target="_blank"}  
[Cheatsheet on RStuido IDE](https://github.com/rstudio/cheatsheets/raw/master/rstudio-ide.pdf){target="_blank"}  
[Other RStudio cheatsheets](https://rstudio.com/resources/cheatsheets/){target="_blank"}   
[Tidyverse](https://www.tidyverse.org/){target="_blank"}   

[Response bias](https://en.wikipedia.org/wiki/Response_bias)  
[Cross-Sectional data](https://en.wikipedia.org/wiki/Cross-sectional_data?oldformat=true)
[Population](https://en.wikipedia.org/wiki/Population?oldformat=true)
[Sample](https://en.wikipedia.org/wiki/Sampling_(statistics)?oldformat=true)
[Sampling methods](https://en.wikipedia.org/wiki/Sampling_(statistics)?oldformat=true){target="_blank"} 
[Inference](https://www.britannica.com/science/inference-statistics) 

[American Community Survey (ASC)](https://www.census.gov/programs-surveys/acs)  

See [here](https://en.wikipedia.org/wiki/American_Community_Survey) for more detailed information about the survey  
[Measure of America](https://www.ssrc.org/programs/view/moa/)   
[Social Science Research Council](https://www.ssrc.org/) 
   

[Piping in R](https://cran.r-project.org/web/packages/magrittr/vignettes/magrittr.html){target="_blank"}   
[Writing functions](https://r4ds.had.co.nz/functions.html)   
Also see [this case study](https://opencasestudies.github.io/ocs-bloomberg-vaping-case-study/){target="_blank"}  for more information on writing functions.   
[String manipulation cheatsheet](https://rstudio.com/resources/cheatsheets/){target="_blank"}  
[Table formats](https://en.wikipedia.org/wiki/Wide_and_narrow_data){target="_blank"}

[Regression](https://lindeloev.github.io/tests-as-linear/)    
[simple linear regression](https://en.wikipedia.org/wiki/Simple_linear_regression)   
[monotonic association](https://www.statisticshowto.com/monotonic-relationship/)   
[Kendall rank correlation coefficient](https://en.wikipedia.org/wiki/Kendall_rank_correlation_coefficient?oldformat=true)   
[Null hypothesis](https://en.wikipedia.org/wiki/Null_hypothesis)   
[Alternative hypothesis](https://en.wikipedia.org/wiki/Alternative_hypothesis)   
[Probability](https://en.wikipedia.org/wiki/Probability)   
[one-sided and two-sided hypotheses](https://en.wikipedia.org/wiki/One-_and_two-tailed_tests)   
[Nonparametric](https://en.wikipedia.org/wiki/Nonparametric_statistics)     [Parametric](https://en.wikipedia.org/wiki/Parametric_statistics) 
[significance threshold](https://en.wikipedia.org/wiki/Statistical_significance)  
[Z score](https://en.wikipedia.org/wiki/Standard_score)   
[Z score table](https://socratic.org/questions/5986a3e1b72cff6fd48a5408)  
[Z score to p-value calculator](https://www.socscistatistics.com/pvalues/normaldistribution.aspx)   


[`ggplot2` package](http://ggplot2.tidyverse.org){target="_blank"}    
Please see [this case study](https://opencasestudies.github.io/ocs-bp-co2-emissions/)  for more details on using `ggplot2`    
[grammar of graphics](http://vita.had.co.nz/papers/layered-grammar.html){target="_blank"}   
[`ggplot2` themes](https://ggplot2.tidyverse.org/reference/ggtheme.html){target="_blank"}   
[`directlabels` package methods](http://directlabels.r-forge.r-project.org/docs/index.html){target="_blank"}    
[Hmong people](https://en.wikipedia.org/wiki/Hmong_people)   
[Intersections](https://www.vox.com/the-highlight/2019/5/20/18542843/intersectionality-conservatism-law-race-gender-discrimination)   

[Motivating article for this case study about youth disconnection/opportunity youth](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6243446/)


To learn more about importing and wrangling PDFs using the `pdftools` package see this [case study](https://opencasestudies.github.io/ocs-bp-rural-and-urban-obesity/) and this [case study](https://opencasestudies.github.io/ocs-bp-rural-and-urban-obesity/). 

To learn more about what you can do with the `magick` package see this [vingette](https://cran.r-project.org/web/packages/magick/vignettes/intro.html).   

To learn more about the **Mann-Kendall trend test** see
[here](https://www.statisticshowto.com/mann-kendall-trend-test/) and [here](https://www.statisticshowto.com/wp-content/uploads/2016/08/Mann-Kendall-Analysis-1.pdf).  

To learn more about hypothesis testing, see this [case study](https://opencasestudies.github.io/ocs-bp-rural-and-urban-obesity/).    


<u>**Packages used in this case study:** </u>

Package   | Use in this case study                                                                      
---------- |-------------
[here](https://github.com/jennybc/here_here){target="_blank"}       | to easily load and save data  
[pdftools](https://readr.tidyverse.org/)      | to import PDF documents  
[magick](https://cran.r-project.org/web/packages/magick/vignettes/intro.html#Kernel_convolution)      | for importing images and extracting text from images   
[knitr](https://yihui.org/knitr/) |  for showing images in reports    
[dplyr](https://dplyr.tidyverse.org/){target="_blank"}      | to filter, subset, join, add rows to, and modify the data   
[stringr](https://stringr.tidyverse.org/){target="_blank"}      | to manipulate strings  
[magrittr](https://magrittr.tidyverse.org/){target="_blank"}      | to pipe sequential commands 
[tidyr](https://tidyr.tidyverse.org/){target="_blank"}      | to change the shape or format of tibbles to wide and long, to drop rows with `NA` values, to separate a column into additional columns, and to fill out values based on previous values   
[tibble](https://tibble.tidyverse.org/){target="_blank"}      | to create tibbles    
[ggplot2](https://ggplot2.tidyverse.org/){target="_blank"}      | to create plots  
[directlabels](http://directlabels.r-forge.r-project.org/docs/index.html){target="_blank"}      | to add labels directly to lines in plots  
[cowplot](https://cran.r-project.org/web/packages/cowplot/vignettes/introduction.html){target="_blank"}      | to add images to plots 
[forcats](https://forcats.tidyverse.org/){target="_blank"}      | to reorder factor for plot
[kendall](https://cran.r-project.org/web/packages/Kendall/Kendall.pdf) | to implement the Mann-Kendall trend test in R   
[patchwork](https://github.com/thomasp85/patchwork) | to combine plots

#### {.emphasis_block}

Want to learn more about how to prevent and mitigate youth disconnection?  
Or do you know youths who are disconnected?  

See [the program directory at youth.gov](https://youth.gov/evidence-innovation/program-directory?keywords=&field_pd_factors_risks_tid=413&field_pd_factors_protective_tid=All) and [this program listing](https://goc.maryland.gov/wp-content/uploads/sites/8/2015/10/Program-Models-for-Serving-Opportunity-Youth.pdf) focused on Maryland but including other locations for listings of programs dedicated to re-engaging disconnected youth or preventing disconnection.

Also, see [The Center for Communities That Care](https://www.communitiesthatcare.net/) and the
[PROSPER program](https://extension.psu.edu/promoting-school-community-university-partnerships-to-enhance-resilience) for particular examples.

####

### Acknowledgements

We would like to acknowledge [Tamar Mendelson](https://www.jhsph.edu/faculty/directory/profile/1770/tamar-mendelson) for assisting in framing the major direction of the case study.

We would also like to acknowledge the [Bloomberg American Health Initiative](https://americanhealth.jhu.edu/) for funding this work. 